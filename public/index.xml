<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anh&#39;s Tech Blog</title>
    <link>https://blog.aaanh.com/</link>
    <description>Recent content on Anh&#39;s Tech Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Feb 2023 20:30:00 +0000</lastBuildDate><atom:link href="https://blog.aaanh.com/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>https://blog.aaanh.com/about/</link>
      <pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/about/</guid>
      <description>The name is Anh and&amp;hellip; I hate blog frameworks.</description>
      <content>&lt;h1 id=&#34;the-name-is-anh-and&#34;&gt;The name is Anh and&amp;hellip;&lt;/h1&gt;
&lt;p&gt;I hate blog frameworks.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Switch All My Linux Systems to RHEL from Ubuntu</title>
      <link>https://blog.aaanh.com/posts/from-ubuntu-to-rhel/</link>
      <pubDate>Wed, 22 Feb 2023 20:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/from-ubuntu-to-rhel/</guid>
      <description>Ubuntu keeps annoying the heck out of me. Or maybe I&amp;rsquo;m getting older.</description>
      <content></content>
    </item>
    
    <item>
      <title>Migrating from Google Workspace to Microsoft 365</title>
      <link>https://blog.aaanh.com/posts/migrating-google-microsoft/</link>
      <pubDate>Sun, 12 Feb 2023 18:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/migrating-google-microsoft/</guid>
      <description>Unironically a better solution from the lesser evil.</description>
      <content>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Around April 2021, I created my Google Workspace account and subscribed to the Business Standard plan which cost 15.60 CAD per user per month. The offerings that I sought after the most were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Custom email domain&lt;/li&gt;
&lt;li&gt;&lt;code&gt;2 TB&lt;/code&gt; of storage per user&lt;/li&gt;
&lt;li&gt;and&amp;hellip; I think that&amp;rsquo;s all&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Google productivity suite is already good enough on my Google One subscribed personal account, so it did not really affect my choice of cloud platform that much. Although the unlimited Google Meet with 150 participants sounds good on paper, in reality, I did not and have not touched it, bar 2 - 3 times for student club meetings.&lt;/p&gt;
&lt;p&gt;Given the majority of email users nowadays use Google Mail as their provider, one positive note is that the interactions between users are quite smooth in terms of file sharing on Google Drive, which comes with every account.&lt;/p&gt;
&lt;p&gt;In the beginning of 2022, I switched from the Business Standard plan to the Business Starter plan which cost half the price per month to reduce my spending and in anticipation for the very near-coming market stagnation. This comes with a major reduction for the Drive capacity per user, from &lt;code&gt;2 TB&lt;/code&gt; to a meager &lt;code&gt;30 GB&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Prior to my migration to Microsoft 365 today, there are 3 domains with multiple aliases on Google Workspace.&lt;/p&gt;
&lt;p&gt;After researching the plans offered on Microsoft 365, I have concluded that the benefits of M365 over Workspace.&lt;/p&gt;
&lt;h2 id=&#34;rationale&#34;&gt;Rationale&lt;/h2&gt;
&lt;h3 id=&#34;costs-before-tax-in-canadian-dollars&#34;&gt;Costs (before tax, in Canadian Dollars)&lt;/h3&gt;
&lt;p&gt;Google Workspace (GWS) Business Starter: 7.80 per User per Month&lt;/p&gt;
&lt;p&gt;Microsoft365 (M365) Business Basic: 6.00 per User per Month&lt;/p&gt;
&lt;h3 id=&#34;offerings-cheapest-plan&#34;&gt;Offerings (cheapest plan)&lt;/h3&gt;
&lt;p&gt;These are the offerings that matter to me the most.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Be warned: These are mostly L&amp;rsquo;s for Google&amp;hellip;&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Google&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;M365&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Email Domain&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Email Storage&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Part of Cloud Storage&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;50 GB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Email Aliases (diff. domains)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes (&lt;code&gt;&amp;lt;=&lt;/code&gt; 50)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes (&lt;code&gt;&amp;lt;=&lt;/code&gt; 400)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cloud Storage&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;30 GB&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;1 TB&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Productivity Suite Type&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Web&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Web&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Productivity Suite&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;IYKYK&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Office is better, tbh&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MFA&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes, any authenticator&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes, any auth, best with MSFT&amp;rsquo;s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MFA w. Security Key&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes, but never shows&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MFA w. Number Matching + Location&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;No&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Collaboration (real-time, externally)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;I&amp;rsquo;ll just use Google One&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;A lot to be desired&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Chat app&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Use Discord or Slack, dude&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Teams is so-so but best one out of them all&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cloud Infra&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Google Cloud is a pain&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Azure is also a pain but manageable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Support&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;😂&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;🤔&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Feature documentations&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;I hate them&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Usable&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;As you can see, M365 offers a lot more bang for your bucks at an even cheaper price, so it is completely logical to migrate out of GWS, saving 1.8 dollar a month towards my daily intake of coffee. Especially so when you look at the cloud storage of 1 TB versus a meagre 30 GB (that&amp;rsquo;s 33.33 times the amount of storage).&lt;/p&gt;
&lt;p&gt;Unfortunately for Google, Microsoft has reached the pinnacle of economy of scale. Love or hate Microsoft and their way of collecting user information, it is certainly clear what the more economic choice is.&lt;/p&gt;
&lt;p&gt;Another aspect to consider but I didn&amp;rsquo;t include because I thought it wouldn&amp;rsquo;t matter that much is Single Sign On (SSO) integration with third-party applications. Through what I could observe, there are more applications with Google SSO than those that have Microsoft. However, at the end of the day, there are ways to integrate your own domain SSO through APIs without first-party support (done by the application owner). So, unless it is a major inconvenience to you (lack of internal resource, fundings, etc.), I wouldn&amp;rsquo;t take that into consideration.&lt;/p&gt;
&lt;h2 id=&#34;migration&#34;&gt;Migration&lt;/h2&gt;
&lt;p&gt;The migration process is mostly performed on M365 Admin Center. Everything looked straightforward enough until it wasn&amp;rsquo;t and I think both parties (G and M) are to be blame.&lt;/p&gt;
&lt;p&gt;The first-party migration tool on M365 Admin Center was simple to go through the steps. It involved creating a service account on Google Cloud that can manage Google Workspace in ELI5 terms. An M365 migration application is installed on Google Workspace to use the service account to perform the process. However, during the verification step of the service account&amp;rsquo;s permission, M365 keeps throwing error saying that there&amp;rsquo;s insufficient permission to continue. I had tried everything, going so far as to giving all the permissions I could get my hands on.&lt;/p&gt;
&lt;p&gt;Post-mortem-ly, Google&amp;rsquo;s way of managing the service accounts is too confusing and I was in deeper than I had wanted, reading through countless docs and troubleshooting forum threads. Microsoft, on the other hand, could improve their toolings, but it does disclaim that the tool is still beta.&lt;/p&gt;
&lt;p&gt;In the end, I had to manually migrate everything.&lt;/p&gt;
&lt;h3 id=&#34;cloud-storage&#34;&gt;Cloud Storage&lt;/h3&gt;
&lt;p&gt;This involved downloading the Google Drive data (9 GB total, doable) and exporting all my mails. Nothing to write home about.&lt;/p&gt;
&lt;h3 id=&#34;emails&#34;&gt;Emails&lt;/h3&gt;
&lt;p&gt;For the e-mails, they are exported as a single &lt;code&gt;.mbox&lt;/code&gt; file (all mails concatenated into a single file, including the header). This is widepsread file format on POSIX systems, so Google Mail does the right thing here. However, Microsoft Outlook does not support &lt;code&gt;.mbox&lt;/code&gt; file 😂 Why? Because it uses &lt;code&gt;.eml&lt;/code&gt; which is a format that is developed by Microsoft, duh. &lt;code&gt;.eml&lt;/code&gt; stores each mail in one file, though. Honestly, not that big of a deal, I imported the single &lt;code&gt;.mbox&lt;/code&gt; file to Thunderbird (bless Mozilla) and exported as &lt;code&gt;.eml&lt;/code&gt; files.&lt;/p&gt;
&lt;h3 id=&#34;calendar&#34;&gt;Calendar&lt;/h3&gt;
&lt;p&gt;Nothing special. Exported as &lt;code&gt;.ics&lt;/code&gt;, pretty standard. Outlook can read and import just fine.&lt;/p&gt;
&lt;h3 id=&#34;documents&#34;&gt;Documents&lt;/h3&gt;
&lt;p&gt;Because Drive doesn&amp;rsquo;t let you change ownership to users outside of the domain (a la my-personal-account), every document that I have shared so far to anyone must be manually uploaded to my personal account and re-shared. I made duplicates with my personal drive wherever I can, but I could only do damage control to an already sinking ship. I had to manually re-share the stuff.&lt;/p&gt;
&lt;h2 id=&#34;2-weeks-use-review&#34;&gt;2 Weeks&amp;rsquo; Use Review&lt;/h2&gt;
&lt;p&gt;At this point, I am very much used to Microsoft products and its Azure AD, so I am biased when I say that everything works perfectly fine so far. The ability to use the desktop Office version is a nice-to-have (with the first free month of trial) but not a necessity if all you do is what you would do with Google suite.&lt;/p&gt;
&lt;p&gt;A caveat that I encountered during my domain transfer was that the Exchange Admin Center bugged out. Even though the DNS records had been added for 3 hours, Exchange would still not recognize the domains as authorized. Some of which are quite mission-critical which I transferred last because I could try with my other domains first while maintain the email available should anything be sent my way. I got impatient so I removed the domains that were pending and readded them through M365 Admin Center which did the trick.&lt;/p&gt;
&lt;p&gt;After everything, I&amp;rsquo;d rate Google 6.5 out of 10, Microsoft 8.5 out of 10, migration process 3 out of 10. Would I recommend doing it? Probably not by yourself. Hire a certified professional to do it for you and save the headaches for a security breach or something.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Developing a Photo Reel with REST API Cloud Storage</title>
      <link>https://blog.aaanh.com/posts/api-for-images/</link>
      <pubDate>Sat, 31 Dec 2022 15:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/api-for-images/</guid>
      <description>Gotta show off the nice photos.</description>
      <content>&lt;p&gt;When I have some free time, I sometimes go out and take photos of the sceneries and the urban landscapes with my camera setups. Usually afterwards I would just post on the usual social media, like Facebook or Instagram. After uploading for a while, I have come to the realization that hosting and serving the images on the usual platform is not the best way because the purpose of the content would get diluted by the noisy neighbours.&lt;/p&gt;
&lt;p&gt;I also figured that hosting these on a specialized infrastructure and website would force me to learn new development techniques and knowledge plus the caveats that come with it. So, I set out to build one from scratch.&lt;/p&gt;
&lt;h2 id=&#34;planning&#34;&gt;Planning&lt;/h2&gt;
&lt;p&gt;My planning process is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Figure out where to host the images
&lt;ul&gt;
&lt;li&gt;What requirements the image hosting infra needs to satisfy?
&lt;ul&gt;
&lt;li&gt;Basic GUI for uploading&lt;/li&gt;
&lt;li&gt;Rate limiting&lt;/li&gt;
&lt;li&gt;API support&lt;/li&gt;
&lt;li&gt;Organization: by year, by tag&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The pricing
&lt;ul&gt;
&lt;li&gt;Free-tier (cuz I&amp;rsquo;m always broke 🙏)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;As long as it uses REST API, we&amp;rsquo;re in business&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Developing the frontend gallery
&lt;ul&gt;
&lt;li&gt;Definitely React/Nextjs (at that time, I didn&amp;rsquo;t know anything else)&lt;/li&gt;
&lt;li&gt;Simple styling with vanilla Tailwindcss&lt;/li&gt;
&lt;li&gt;Dynamically loading to save bandwidth and improve performance&lt;/li&gt;
&lt;li&gt;Dynamically render the images according to the &lt;code&gt;year&lt;/code&gt; tag&lt;/li&gt;
&lt;li&gt;Retrieve the image list from the hosting platform&lt;/li&gt;
&lt;li&gt;Simple gallery that renders the image in auto-sized columns&lt;/li&gt;
&lt;li&gt;Hover image to zoom in&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;development&#34;&gt;Development&lt;/h2&gt;
&lt;p&gt;For serving an image to a visitor of your website, hosting the images is only half of the equation. Accessing a hosted resource on the cloud can simply be done with a HTTP request even from a terminal shell. Now, a low-tech option is hosting it on a baremetal server off the coast of the Bahamas and serve the assets via static URL with an Apache or NGINX server. Graduating from priamry school, I&amp;rsquo;d get into hosting a database, more specifically in this case, a NoSQL database for the images. But then, to communicate with the database, I&amp;rsquo;d need to develop a backend server that supports CRUD operations and REST API, not to mention I&amp;rsquo;d also have to model the object storage. At this point, I got too lazy so the best thing I had in mind is to outsource that whole headaches to some SaaS products. These SaaS products would usually come with a Content Delivery Network (CDN). &lt;a href=&#34;https://en.wikipedia.org/wiki/Content_delivery_network&#34;&gt;Read more&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After researching for an image hosting platform and disenchanted by the operational overheading of self-hosting a database + backend, I narrowed down to 2 options: Cloudflare and Cloudinary. Cloudflare, unfortunately as much as I like it, does not have a free-tier plan for images hosting and serving. So, I went with Cloudinary &amp;ndash; which meant that I had to learn a whole new platform from the start.&lt;/p&gt;
&lt;p&gt;I created a free account on Cloudinary and set up the stuff. First impression, the UI is a bit busy, but nowhere near the confundus puddle of confusion on AWS. I uploaded some images and tagged them and use the unique API URL to query the data. Everything seemed to go smoothly. The endpoint I tested returned a JSON object containing the all the uploaded images. See the following example.&lt;/p&gt;
&lt;p&gt;The cURL request&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl --location --request GET &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://api.cloudinary.com/v1_1/some_cloud_name/resources/image&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--header &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Authorization: Basic SOME=64BIT=ENCODED=SECRET&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The response body&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;resources&amp;#34;&lt;/span&gt;: [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;asset_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;some_hex_id&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;public_id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2022/some_file_name.jpg&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;format&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;jpg&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;version&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1672194970&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;resource_type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;image&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;upload&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;created_at&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2022-12-28T02:36:10Z&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;bytes&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1153399&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;width&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;4032&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;height&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3024&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;folder&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2022&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;url&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http://res.cloudinary.com/some_cloud_name/image/upload/some_unique_id/2022/some_file_name.jpg&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;secure_url&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://res.cloudinary.com/some_cloud_name/image/upload/some_unique_id/2022/some_file_name.jpg&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      },
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;LGTM! So I dug further into the documentation and found out how to query by tag &amp;ndash; which is important for me to query the images by year. I also noticed the &amp;ldquo;Transformations&amp;rdquo; doc and decided to check out what it is. To my pleasant surprise, Cloudinary has a feature where you can add query params to the request and it would respond with the resized images.&lt;/p&gt;
&lt;p&gt;Then, I crafted a GET request with the fetch API + useSWR on the Nextjs application. Like so&amp;hellip;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-jsx&#34; data-lang=&#34;jsx&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;default&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;async&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;req&lt;/span&gt;, &lt;span style=&#34;color:#a6e22e&#34;&gt;res&lt;/span&gt;) =&amp;gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;response&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;await&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;getStaticProps&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;req&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;query&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;year&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;data&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;response&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;props&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;images&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;resources&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;map&lt;/span&gt;((&lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;) =&amp;gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;url&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;secure_url&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;substring&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/w_500,c_scale&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;secure_url&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;substring&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;)),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;public_id&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;public_id&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;folder&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;folder&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;width&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;width&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;height&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;height&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#a6e22e&#34;&gt;format&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;image&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;format&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  })
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;res&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;status&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;).&lt;span style=&#34;color:#a6e22e&#34;&gt;json&lt;/span&gt;(&lt;span style=&#34;color:#a6e22e&#34;&gt;data&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This acts as a middleware that queries my Cloudinary library for images, does some magical string operations that I cooked up and have now forgotten how to generate the correct URL, and outputs that to the Nextjs API endpoint. So every time, I want to retrieve the data, I could fetch the application URL instead of going directly to the Cloudinary endpoint which would save me on the number of requests.&lt;/p&gt;
&lt;p&gt;After verifying that the images matching the query are soundly returned in the response body, all that was left was to design the frontend for which I mushed and mashed together some HTML code and Tailwindcss classes. Bing, bam, boom.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In terms of performance, I have not got to optimizing that much but the website should load almost instantly on first visit because of the minimal size of frontend code that I used for the design. The images on the other hand took quite a bit more, on average 1-2 seconds for the images in a tagged year to load.&lt;/p&gt;
&lt;p&gt;I have never reached the rate limitation of the free-tier quota and averages at around 1% plan usage per month. The images were exported in JPG format with Best Quality selected in Lightroom and average at around 1MB each for a running total of ~35 images rendered on the frontend. Disregarding the intial upload size, the images are already transformed into even smaller files in size at around 500KB.&lt;/p&gt;
&lt;p&gt;Visit the photo gallery at: &lt;a href=&#34;https://photos.aaanh.ca&#34;&gt;https://photos.aaanh.ca&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Source code is available at: [ &lt;a href=&#34;https://github.com/aaanh/my-photo-reel&#34;&gt;Github&lt;/a&gt; ]&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Rendering Static HTML from LaTeX Source: A Madness-Inducing Adventure</title>
      <link>https://blog.aaanh.com/posts/rendering-html-from-latex/</link>
      <pubDate>Sun, 30 Oct 2022 20:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/rendering-html-from-latex/</guid>
      <description>LaTeX is popular but everything about it is so far behind.</description>
      <content></content>
    </item>
    
    <item>
      <title>A Cute and Neat Serverless Function for Github Gists</title>
      <link>https://blog.aaanh.com/posts/serverless-functions/</link>
      <pubDate>Sat, 29 Oct 2022 22:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/serverless-functions/</guid>
      <description>Going in raw.</description>
      <content></content>
    </item>
    
    <item>
      <title>Setting Up a Barebone Dedicated ML Server</title>
      <link>https://blog.aaanh.com/posts/setting-up-a-barebone-ml-server/</link>
      <pubDate>Sat, 23 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/setting-up-a-barebone-ml-server/</guid>
      <description>I sure hope that I will make-even with this investment 💸</description>
      <content>&lt;h1 id=&#34;the-purchase&#34;&gt;The Purchase&lt;/h1&gt;
&lt;p&gt;Back in July 2, after a bi-weekly report on my research progress to the group, I headed to eBay to buy a dedicated GPU for the machine learning tasks that I am performing. The reason, to quell my predictable buyer&amp;rsquo;s remorse, was to have a separate accelerator card from my workstation that runs on my homelab server instead. This card would need to have a (more) massive physical memory than the measly 8GB that my GTX 1080 has and quite a lot cheaper to actually justify the purchase.&lt;/p&gt;
&lt;p&gt;After digging around for few hours, I stumbled upon the NVidia Tesla M40 GPU. It is a data-center class GPU based on the &lt;strong&gt;M&lt;/strong&gt;axwell architecture.&lt;/p&gt;
&lt;p&gt;Here are the specifications:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Attribute&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;GPU Architecture&lt;/td&gt;
&lt;td&gt;NVIDIA Maxwell&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CUDA Cores&lt;/td&gt;
&lt;td&gt;3072&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Single-Precision Perf&lt;/td&gt;
&lt;td&gt;7 TFlops w/ NVIDIA GPU Boost&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Double-Precision Perf&lt;/td&gt;
&lt;td&gt;0.2 TFlops&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GPU Memory&lt;/td&gt;
&lt;td&gt;24 GB GDDR5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Memory Bandwidth&lt;/td&gt;
&lt;td&gt;288 GB/s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Interface&lt;/td&gt;
&lt;td&gt;PCIe 3.0 x16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Max Power Consumption&lt;/td&gt;
&lt;td&gt;250 W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cooling&lt;/td&gt;
&lt;td&gt;Passive&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;While it is a 7-years-old GPU, considering the price of &lt;code&gt;US$179&lt;/code&gt; and three-times the GPU memory of my GTX 1080, I was quite sold on the idea of getting it. The very significant outcome is that I would be able to train on larger batch sizes and perform more aggressive feature engineering tasks.&lt;/p&gt;
&lt;h1 id=&#34;the-delivery-rant-skip-ahead&#34;&gt;The Delivery (rant, skip ahead)&lt;/h1&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;I smelled an absolute dogshit stench combined with fermented fishiness that it was utter horseshit. First, it came from the fact that I had never provided any signature for any delivery. Second, there was no delivery attempt, no phone calls, no SMS. But I checked the vicinity of the house anyway just to be sure. To no one&amp;rsquo;s surprise, it wasn&amp;rsquo;t there. I checked the tracking on the website and it still said delivered. WTF?&lt;/p&gt;
&lt;p&gt;The next morning, I called Intelcom&amp;rsquo;s customer support. It was not easy finding the customer support number either, because there was (and I guarantee still is) nothing listed on the official website. What a fucking shady business that operates on anti-consumerism. Anyway, I waited 20 minutes on a line until I finally got transferred to an agent. The problem seemed to be an incorrect shipping address on the system &lt;code&gt;?? 🙂 ??&lt;/code&gt;. Whatever I&amp;rsquo;d just provide again the correct address and it&amp;rsquo;d be good (right?) and the delivery was still scheduled for Monday. Came Monday, an e-mail came around 4:00 PM saying that the GPU would be delivered from 5:00 PM to 10:00 PM. Said time period came, and I got a call from the delivery person asking for the correct address. What the fuck? Didn&amp;rsquo;t I just correct it on Sunday? Was I sleep-dialing customer support in my dream? Okay fine, I gave the person the correct address and the motherfucker literally had the gall to say that it was too far away (30 minutes) from his location so he didn&amp;rsquo;t want to deliver (????). I&amp;rsquo;m sorry, but ain&amp;rsquo;t the delivery time between 5 to 10? It was fucking 5:30 PM at that time.&lt;/p&gt;
&lt;p&gt;To be abridged and shortened, I dealt with customer support for 2 additional times on the phone, each lasting around 30-40 minutes just to harass them to do the right thing or else I would report their ass to kingdom come with Quebec Consumer Protection Office and drag their faces through the mud on Better Business Bureau. And it motherfucking worked. I finally got the GPU on Thursday.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;the-installation&#34;&gt;The Installation&lt;/h1&gt;
&lt;p&gt;After installing the card in my server, I realized that the PSU didn&amp;rsquo;t have enough power (450 W) to run the whole system with the M40 GPU. So, I ran to the nearest Canada Computers and picked up a 650 W PSU. I had to saw off the second CPU power connector clip in order to fit the card&amp;rsquo;s power slot. And&amp;hellip; It still didn&amp;rsquo;t work.&lt;/p&gt;
&lt;p&gt;Perusing the great interweb for an hour or so, I came across a YouTube video that said I would need to enable something-something more than 4-GB decoding(?) in BIOS for it to work. The main problem was I had been running the server headlessly. That is, only through SSH and no user I/O whatsoever.&lt;/p&gt;
&lt;p&gt;To solve this, I devised 2 options:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Buy a CPU with iGPU (my current one doesn&amp;rsquo;t have one)&lt;/li&gt;
&lt;li&gt;Buy a cheap-ass GPU&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For option, the most sensible thing was to get a Ryzen 3200G (I am running on an AMD motherboard). But it costs around 300 dollary-doos. On the other hand, a cheap-ass GPU would only cost me 99 Canadian rupees in total, so I went for the latter and bought a Radeon card off Amazon.&lt;/p&gt;
&lt;p&gt;Card arrived. BIOS configured. System booted to OS. And working fine.&lt;/p&gt;
&lt;h1 id=&#34;the-nvidia-installation&#34;&gt;The NVIDIA Installation&lt;/h1&gt;
&lt;p&gt;The card would totally not work if there ain&amp;rsquo;t any drivers, especially for machine learning that utilizes those sweet CUDA cores.&lt;/p&gt;
&lt;p&gt;I first disabled the default Nouveau driver:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo su
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;touch /etc/modprobe.d/blacklist-nouveau.conf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;echo -e &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;blacklist nouveau\noptions nouveau modeset=0&amp;#34;&lt;/span&gt; &amp;gt;&amp;gt; /etc/modprobe.d/blacklist-nouveau.conf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;update-initramfs -u
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;reboot
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, I installed the CUDA Toolkit using the Runfile method for easy clap:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda_11.7.0_515.43.04_linux.run
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo sh cuda_11.7.0_515.43.04_linux.run
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, I installed &lt;code&gt;anaconda&lt;/code&gt; (for demo only, please refer to Anaconda official website for latest downloads):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; bash Anaconda3-2022.05-Linux-x86_64.sh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After that, I created a new environment, installed Jupyter Notebook and dependencies. And finally, I ran the first ever training (pictured below). I made sure to monitor the GPU usage through &lt;code&gt;nvidia-smi&lt;/code&gt; to verify if it was utilizing the GPU, not the CPU.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h1 id=&#34;afterwords&#34;&gt;Afterwords&lt;/h1&gt;
&lt;p&gt;Looking at the whole process, I spent around &lt;code&gt;CA$350&lt;/code&gt; in total for the GPU + compatible components + other efforts. Comparing that one-time price tag (electricity excluded) to Google Colab (&lt;code&gt;CA$15&lt;/code&gt; a month if I remember correctly), I should be able to breakeven in 2 years. However, it should be taken into considerations that Google Colab has timeouts and the GPU instance provided is not guaranteed, so on a good day, you might get A100 or V100, but on a bad day, you would get the whimpy K80. Adding more to the variance, you are also dependent on the storage space of your Google Drive to store models and data, which adds 5 - 15 dollars a month for a usable amount of storage. Thus, bringing down the breakeven period to simply less than 1 year of continuous usage.&lt;/p&gt;
&lt;p&gt;In the near future, I would want to experiment with distributed machine learning setup between the server and my workstation to see if it has any larger potential with performance gain.&lt;/p&gt;
&lt;p&gt;Subjectively, I think the effort was worthwhile. Though, it would have been way better if it was not for Intelcom. I&amp;rsquo;m calling that shitty company out. Keep that in your mind: Intelcom is bad.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>When you can be a 2D Anime Waifu Oshi</title>
      <link>https://blog.aaanh.com/posts/why-be-a-king/</link>
      <pubDate>Fri, 08 Jul 2022 12:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/why-be-a-king/</guid>
      <description>Democratizing virtual spaces technologies.</description>
      <content>&lt;p&gt;I did the thing. Armed with my iPad (and iPhone) and a PC desktop, along with a general knowledge of how computer works.&lt;/p&gt;
&lt;p&gt;The setup does not really require an Apple device but it works best because of Apple AR Kit.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.facebook.com/100004982768603/videos/820917322223296/&#34;&gt;Watch the test recording here&lt;/a&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;I just discovered Excalidraw in May and I could not stop using it. It&amp;rsquo;s an addiction. So here&amp;rsquo;s a diagram of the &lt;em&gt;pipeline&lt;/em&gt;.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Thanks.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Building Towards a Taxonomy of MLaaS</title>
      <link>https://blog.aaanh.com/posts/building-towards-mlaas-taxonomy/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/building-towards-mlaas-taxonomy/</guid>
      <description>There is a scarcity in terms of papers 😣 so I have to get creative 💡</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;⚠ Under Construction. Please check back later or help yourself with my previous publications 🤗.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This post is adapted on my internal report #3, as part of my research group efforts in studying and developing machine learning processes, frameworks, and knowledge base. In addition to the adaptation of the report, I add further context for general viewership comprehension.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;the-building-blocks-or-components&#34;&gt;The Building Blocks (or Components)&lt;/h2&gt;
&lt;h3 id=&#34;service-oriented-architecture&#34;&gt;Service-Oriented Architecture&lt;/h3&gt;
&lt;h3 id=&#34;service-component-architecture&#34;&gt;Service Component Architecture&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;According to the &lt;a href=&#34;http://www.oasis-opencsa.org&#34;&gt;OASIS Open Composite Services Architecture&lt;/a&gt;, SCA comprises 3 attributes(?)/specifications(?):
&lt;ul&gt;
&lt;li&gt;Components&lt;/li&gt;
&lt;li&gt;Composites&lt;/li&gt;
&lt;li&gt;Services&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The components implement its business function; the composites assemble multiple compnents together to create business solutions; and the services create an interface for the user with the component and composite functions.&lt;/p&gt;
&lt;p&gt;So, we can apply such concept to reach an understanding that any MLaaS is built upon many functional components, be them data-related or training task-related. The user can pick and choose any or all the functional components to achieve their goals via a unified interface, e.g. a web console.&lt;/p&gt;
&lt;p&gt;Building further on this concept, providers can target different market segments and technical levels while also easily maintain the contractual service level agreements (SLA) and define key service level objectives (SLO). This homing-in is achieved through the modularity and scalability of the service product.&lt;/p&gt;
&lt;h3 id=&#34;attributes-of-a-mlaas&#34;&gt;Attributes of a MLaaS&lt;/h3&gt;
&lt;p&gt;Machine Learning as a Service (MLaaS) from a high level perspective is very much similar to Platform as a Service (PaaS). It is evident by the following observabile-at-a-glance features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is a cloud-based service&lt;/li&gt;
&lt;li&gt;Accessible through web-based (RESTful) UI or API calls from a CLI tool&lt;/li&gt;
&lt;li&gt;Scalable based on the workload
&lt;ul&gt;
&lt;li&gt;Compute resources&lt;/li&gt;
&lt;li&gt;Database size&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Data security, privacy provisions&lt;/li&gt;
&lt;li&gt;High availability and fault tolerant&lt;/li&gt;
&lt;li&gt;Automation-friendly, which is crucial for MLOps&lt;/li&gt;
&lt;li&gt;Offering flexible pricing models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we lay the foundation for MLaaS roughly can be. One might pose the question, well then, how do we differentiate MLaaS from other types of services? We might want to consider viewing it as being encapsulated by the PaaS and offering more specialized tools and functions that non-ML platforms. Or MLaaS is at the same layer/level of a PaaS and the sole job of MLaaS is its namesakes. Maybe we can consider the types of models that a service provide or the availability of an ML model itself.&lt;/p&gt;
&lt;h2 id=&#34;putting-the-blocks-together&#34;&gt;Putting the Blocks Together&lt;/h2&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;Something something&lt;/p&gt;
&lt;h2 id=&#34;in-reality-tesla&#34;&gt;In Reality (Tesla)&lt;/h2&gt;
&lt;h2 id=&#34;going-back-to-our-research-interests&#34;&gt;Going Back to Our Research Interests&lt;/h2&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.oasis-opencsa.org/sca&#34;&gt;Service Component Architecture (SCA). Edwards, Oasis-OpenCSA.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1SGRBGFTs7QGNBfOI59L3PovjtqN7RuNK&amp;amp;authuser=iam%40hoanganh.tech&amp;amp;usp=drive_fs&#34;&gt;MLModelCI: An Automatic Cloud Platform for Efficient MLaaS. Zhang et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=10KdimmnGQtjReA8bKaUmzBKeJ1icczN0&amp;amp;authuser=iam%40hoanganh.tech&amp;amp;usp=drive_fs&#34;&gt;MLaaS: Machine Learning as a Service. Ribeiro et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=1-qRhjVQisle7tfRoZIiqVtffloUO5Air&amp;amp;authuser=iam%40hoanganh.tech&amp;amp;usp=drive_fs&#34;&gt;Patent “Data Pipeline and Deep Learning System for Autonomous Driving”. Uvarov, Tesla Inc.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Ucp0TTmvqOE&amp;amp;t=7760s&amp;amp;ab_channel=Tesla&#34;&gt;Andrej Karpathy presentation at Tesla Autonomy Day, Tesla on YouTube.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oBklltKXtDE&amp;amp;t=507s&amp;amp;ab_channel=PyTorch&#34;&gt;Andrey Karpathy presentation at PyTorch at Tesla, PyTorch on YouTube.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Holocure Save File Restore</title>
      <link>https://blog.aaanh.com/posts/holocure-save-file/</link>
      <pubDate>Sun, 03 Jul 2022 04:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/holocure-save-file/</guid>
      <description>Had insomnia last night so I stayed up until dawn playing Holocure</description>
      <content>&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://kay-yu.itch.io/holocure&#34;&gt;Holocure&lt;/a&gt; is a fan-made content by the animator &lt;a href=&#34;https://twitter.com/kaynimatic&#34;&gt;Kay Yu&lt;/a&gt; in the form of a survival, roguelike, shoot &#39;em up game. The gameplay mechanics are similar to those of the game &lt;a href=&#34;https://store.steampowered.com/app/1794680/Vampire_Survivors/&#34;&gt;Vampire Survivors&lt;/a&gt;. The game content features &lt;a href=&#34;https://hololive.hololivepro.com/&#34;&gt;Hololive&lt;/a&gt; characters, who are Virtual YouTubers (vtubers), along with their references and signature items.&lt;/p&gt;
&lt;p&gt;The game target platform is &lt;code&gt;win32&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;problem-statements&#34;&gt;Problem Statements&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;You want to continue your Holocure progress on another Windows machine.&lt;/li&gt;
&lt;li&gt;You have a backed up save file &lt;code&gt;save.dat&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;You can only download the latest version of Holocure.
&lt;ul&gt;
&lt;li&gt;You don&amp;rsquo;t have access to the version of assemblies that generated the &lt;code&gt;save.dat&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You don&amp;rsquo;t trust scripts and tools on the Interweb, preferring to do it manually.
&lt;ul&gt;
&lt;li&gt;You are somewhat competent at more technical computing concepts &amp;amp; operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;methodology&#34;&gt;Methodology&lt;/h1&gt;
&lt;h2 id=&#34;restore-steps&#34;&gt;Restore Steps&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;On your brand spanking new machine&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate the initial save folder
&lt;ol&gt;
&lt;li&gt;Download the game from official release channel &lt;a href=&#34;https://kay-yu.itch.io/holocure&#34;&gt;https://kay-yu.itch.io/holocure&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Launch the game for the first time.&lt;/li&gt;
&lt;li&gt;Get to the game main menu.&lt;/li&gt;
&lt;li&gt;Exit it.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Get content of new &lt;code&gt;save.dat&lt;/code&gt; file. All steps in this section are performed on the new save directory that the game generates by default.
&lt;ol&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;Win&lt;!-- raw HTML omitted --&gt; + &lt;!-- raw HTML omitted --&gt;R&lt;!-- raw HTML omitted --&gt;{&amp;rsquo; &amp;lsquo;}&lt;/li&gt;
&lt;li&gt;Paste this path in and hit &lt;!-- raw HTML omitted --&gt;Enter&lt;!-- raw HTML omitted --&gt;:
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;%USERPROFILE%\AppData\Local\Holocure
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;⚠ Make a backup immediately ⚠
&lt;ul&gt;
&lt;li&gt;Right-click &lt;code&gt;save.dat&lt;/code&gt; &amp;gt; Copy &amp;gt; &lt;!-- raw HTML omitted --&gt;CTRL&lt;!-- raw HTML omitted --&gt; + &lt;!-- raw HTML omitted --&gt;V&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;Name the copy &lt;code&gt;save.dat.bak&lt;/code&gt; or something&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Open &lt;code&gt;save.dat&lt;/code&gt; file there with a text editor
&lt;ul&gt;
&lt;li&gt;save.dat: ASCII text, with very long lines (1560), with no line terminators&lt;/li&gt;
&lt;li&gt;The file is encoded in base64&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Copy all the content and paste it into &lt;a href=&#34;https://www.base64decode.org/&#34;&gt;a decoder like base64&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Or use &lt;code&gt;base64&lt;/code&gt; CLI in WSL &lt;code&gt;base64 -d save.dat &amp;gt;&amp;gt; save.decoded.&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sample decoded file:
&lt;ul&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You are probably seeing a string like this:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;y&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;ݴouy&lt;/span&gt;{&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;F{sm&lt;/span&gt;}&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;^svi&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;s&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;}&lt;/span&gt;{&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Vk{&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;food&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;specUnlock&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;haste&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;2.0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;holoCoins&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;-136.0&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;unlockedItems&amp;#34;&lt;/span&gt;: [ &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BodyPillow&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FullMeal&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;PikiPikiPiman&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;SuccubusHorn&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Headphones&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;UberSheep&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;........&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Look for &lt;code&gt;randomMoneyKey&lt;/code&gt; attribute and write down its the numeric value
&lt;ul&gt;
&lt;li&gt;Example: &lt;code&gt;&amp;quot;randomMoneyKey&amp;quot;: 420.0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;This is a randomly generated &amp;ldquo;key&amp;rdquo; that acts as an anti-cheat for the money you have.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Pay attention to the first non-sensical word in double quotes you can see
&lt;ul&gt;
&lt;li&gt;In this case, &lt;code&gt;&amp;quot;food&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Trace backwards to the closest open curly bracket &lt;code&gt;{&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy the portion from and including that bracket till the end and including the weird box character.&lt;/li&gt;
&lt;li&gt;Paste into &lt;a href=&#34;https://www.base64encode.org/&#34;&gt;the tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Re-encode the string segment into base64
&lt;ul&gt;
&lt;li&gt;You get something like &lt;code&gt;eyAiZm9vZCI6........&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;This gives you the starting point of the actual saved progress of the game.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;So, back in the &lt;code&gt;save.dat&lt;/code&gt;, find where the segment you got from the previous step is and yeet that.&lt;/li&gt;
&lt;li&gt;Save the modified &lt;code&gt;save.dat&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;This step concludes the preparation of the local save directory.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Prepping the backed up &lt;code&gt;save.dat&lt;/code&gt; file containing your progress
&lt;ol&gt;
&lt;li&gt;Download or transfer the file to your new machine.&lt;/li&gt;
&lt;li&gt;For concision, this guide assumes you paste the &lt;code&gt;save.dat&lt;/code&gt; file on your &lt;code&gt;%USERPROFILE%\Desktop&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Open the file with a text editor&lt;/li&gt;
&lt;li&gt;Decode it&lt;/li&gt;
&lt;li&gt;Look for the &lt;code&gt;randomMoneyKey&lt;/code&gt; attribute&lt;/li&gt;
&lt;li&gt;Change its value to the number you got from the default file at &lt;code&gt;%USERPROFILE%\AppData\Local\Holocure\save.dat&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;In this case, &lt;code&gt;420.0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;With that value change, like the logic you did with the default file
&lt;ul&gt;
&lt;li&gt;Find the first non-sensical word in that string&lt;/li&gt;
&lt;li&gt;Trace backwards to the closest open curly bracket &lt;code&gt;{&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy the portion from and including that bracket till the end and including the weird box character.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Paste the copied string to &lt;a href=&#34;https://base64encode.org&#34;&gt;the encoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Re-encode it.&lt;/li&gt;
&lt;li&gt;Copy the re-encoded string.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Back to the directory &lt;code&gt;%USERPROFILE%\AppData\Local\Holocure\&lt;/code&gt;
&lt;ol&gt;
&lt;li&gt;Open the &lt;code&gt;save.dat&lt;/code&gt; file if you have not already.&lt;/li&gt;
&lt;li&gt;Paste (append) the copied string right after whatever is there, make sure there&amp;rsquo;s no whitespace &lt;!-- raw HTML omitted --&gt;Space&lt;!-- raw HTML omitted --&gt;.&lt;/li&gt;
&lt;li&gt;Save it.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Re-open HoloCure&lt;/li&gt;
&lt;li&gt;Et voila, it is back.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;backup-strategies&#34;&gt;Backup Strategies&lt;/h2&gt;
&lt;h3 id=&#34;cloud-sync&#34;&gt;Cloud Sync&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I use Google Drive to automatically back up that specific folder
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;%USERPROFILE%\AppData\Local\Holocure
&lt;/code&gt;&lt;/pre&gt;Since I have Google Drive desktop installed already on all my computers.&lt;/li&gt;
&lt;li&gt;But, to be noted that you would only need the &lt;code&gt;save.dat&lt;/code&gt; file to properly restore your progress.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shadow-copy&#34;&gt;Shadow Copy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;You can make a shadow copy of the folder.&lt;/li&gt;
&lt;li&gt;Won&amp;rsquo;t go into details, there are guides out there on the internet.&lt;/li&gt;
&lt;li&gt;Copy that blob to your new machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;practice-3-2-1&#34;&gt;Practice 3-2-1&lt;/h3&gt;
&lt;p&gt;High resilient data recovery strategy in the event of any type of disaster, except Earth&amp;rsquo;s complete destruction.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 daily backups.&lt;/li&gt;
&lt;li&gt;2 on-site (different machine/server, on a NAS, on a cold storage medium like tape, on a USB key).&lt;/li&gt;
&lt;li&gt;1 off-site (preferably different region).&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;backstory&#34;&gt;Backstory&lt;/h1&gt;
&lt;p&gt;Because I wanted to continue my Holocure progress on my laptop while lying on my bed, I needed to transfer the progress from my desktop tower. That&amp;rsquo;s it, lol 😂.&lt;/p&gt;
&lt;h1 id=&#34;contact-assistance-request-bug-report&#34;&gt;Contact, assistance request, bug report&lt;/h1&gt;
&lt;p&gt;ℹ Please submit an issue at &lt;a href=&#34;https://github.com/aaanh/ml/issues&#34;&gt;https://github.com/aaanh/ml/issues&lt;/a&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Google Cloud Platform Bonanza - Cloud Run</title>
      <link>https://blog.aaanh.com/posts/google-cloud-run-bonanza/</link>
      <pubDate>Sat, 02 Jul 2022 00:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/google-cloud-run-bonanza/</guid>
      <description>Why do you have to be this way?</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Curse-loaded rant alert.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com&#34;&gt;Google Cloud Platform&lt;/a&gt; is Google&amp;rsquo;s the direct competitor to &lt;a href=&#34;azure.com&#34;&gt;Microsoft Azure&lt;/a&gt; and &lt;a href=&#34;https://aws.amazon.com&#34;&gt;Amazon AWS&lt;/a&gt;. Among the services GCP offers is the Cloud Run API. Cloud Run hosts containerized applications, APIs, and microservices while being jam-packed with features like security, access control, port forwarding, loggings, and etc.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;
&lt;em&gt;Fig 1. The dashboard of a deployed Cloud Run project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Cloud Run service and platform are able to differentiate themselves from the others by offering a relatively modern and clean console which helps a lot with navigating through the options and functionalities. Compared to GCP, AWS and Azure seem clunky and the UI flow does not offer a clear path for a developer to deploy their projects. Most importantly, when enabling these services, AWS and Azure have complicated billing account assignment that just throws the user off track.&lt;/p&gt;
&lt;h1 id=&#34;the-spiral-into-despair-begins&#34;&gt;The spiral into despair begins&lt;/h1&gt;
&lt;p&gt;But Cloud Run is not without its own flaws. Very annoying ones at that. In my use case, I have a container of a Fresh Deno app (you can visit it &lt;a href=&#34;https://fresh.hoanganh.dev&#34;&gt;here&lt;/a&gt;) that I want to deploy with Cloud Run. It is web application framework that uses Deno (instead of node) as its runtime. Other than that, it is basically a single-page application. I started by building my container image locally and tagging with the proper scheme. Then, I used the &lt;code&gt;gcloud&lt;/code&gt; CLI to integrate the Google Container Registry with Docker so that when I push an image with the correct tagging scheme, it will be pushed to the registry without much overhead of managing accesses and tokens. Smooth sailing so far.&lt;/p&gt;
&lt;p&gt;After it has been uploaded to the registry, I can use the Cloud Run API console to deploy the container. With the default options set, unauthenticated web access granted, and proper port forwarded, off the service goes. First thing, there is no way to see what the API is doing with the image. The log verbosity is non-existent and the log tab is only for operational logs. So, no actual progress bar whatsoever and you would need to have your fingers crossed that nothing goes wrong. This happened to me the last time when I tried to deploy my &lt;a href=&#34;https://unix.hoanganh.dev&#34;&gt;Unix documentation site&lt;/a&gt;. The deployment went under and I did not know why because the error message was simply a build error because of unsupported architecture. After doing differential testings, I was able to isolate that the problem was due to the base image architecture. See, I was using my M1 MacBook to build and push the image so the base Ubuntu image was on ARM64 architecture. This was subsequently solved by going over the process on my x64 machine.&lt;/p&gt;
&lt;p&gt;Second, there is an option to assign a custom domain to the deployed container and this step is also a major pain. While the assignment of the custom domain went quickly enough, the lack of observability on the progresss is absolutely nerve-wracking due to the undesirably slow DNS propagation and SSL certificate generation. Mind you that I use Cloudflare. So, in my quest to find a better way to do custom domain (because the default &lt;code&gt;*.web.app&lt;/code&gt; is &lt;em&gt;kinda&lt;/em&gt; hard to remember 🤡), I stumbled upon Firebase hosting with redirection.&lt;/p&gt;
&lt;h1 id=&#34;firebase-kinda-sucks-now&#34;&gt;Firebase kinda sucks now&lt;/h1&gt;
&lt;p&gt;Now, Firebase is a seemingly separated service component from all the over GCP stuff. But you can connect Firebase to your GCP project. BUT. I didn&amp;rsquo;t know that was a thing, so I naively created a new project with the hope of being able to redirect to the container service of another project 🤡. Fumbled around a while, I managed to connect the GCP project of the deployed container to Firebase, but then it automatically upgraded the plan to the Blaze paid plan because &amp;ldquo;extend to use Google Cloud Platform resources&amp;rdquo; for whatever reasons. Well, ok, fine, I can live with that as long as the process takes less time. Boy, was I wrong, as I could not implement the redirection even though I followed the documentations and referenced other deeply nested docs.&lt;/p&gt;
&lt;p&gt;All right, I give up. Let&amp;rsquo;s abandon this and disable the Firebase then. Google says, &amp;ldquo;Fuck no.&amp;rdquo; This project is now permanently attached to the Firebase shit and there are no options to detach. I attempted to downgrade the Firebase to the free plan with hope that it would do the trick only to have it RE-ENABLED automatically by the system. At this point, the last thing to do is to delete the project from Firebase console and you&amp;rsquo;d think that is &amp;ldquo;da wey&amp;rdquo; right? Fuck no, again, you silly goof. Delete the project from Firebase also means your whole Google Cloud Platform project would be deleted. Trying to get support for this shit doesn&amp;rsquo;t work because it is not available for the basic tier support 🤡.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;THE FUCK SHIT IS THIS?
I &lt;em&gt;calmly&lt;/em&gt; asked myself and Google.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;last-ditch-efforts&#34;&gt;Last ditch efforts&lt;/h1&gt;
&lt;p&gt;Ok, whatever, then I&amp;rsquo;ll just remove the service accounts in IAM. Wrong again, idiot with a foolish ambition. Removing those automatically created service accounts also somehow messed up the whole Cloud Run API access permissions. Now, I can&amp;rsquo;t even create a new deployment from the web console nor deploy a revision of the containerized app. Trying to solve this via the IAM console is another pain. Google in its infinite engineering wisdom does not provide a clear way of how administrators can manage access of default service agents and their roles. After 1-1.5 hour of reading docs, forums, community pages, I gave up and had to disable the whole API, losing all of my work, and start over.&lt;/p&gt;
&lt;h1 id=&#34;resolution&#34;&gt;Resolution&lt;/h1&gt;
&lt;p&gt;Starting over actually did the trick. The access policies were restored and I was able to re-deploy everything through the web console. HOWEVER, IT DID NOT HAVE TO BE THIS WAY. The time and effort cost was simply unjustified for such simple administrative operation. And it is not like the service is free or anything, I (will) pay for whatever I am consuming, so the shitty docs and non-existent support can go fuck themselves. Twice. Google engineering might be the best, but the customer service and business relation is clearly dogshit. If anything, I will probably sell my soul to Microsoft in the future.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Initiate Exploratory Research on ML/DL/AIaaS</title>
      <link>https://blog.aaanh.com/posts/exploratory-research-on-mlaas/</link>
      <pubDate>Thu, 30 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/exploratory-research-on-mlaas/</guid>
      <description>There are so many of them out there 😱</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;This post is adapted on my internal report #1 and #2, as part of my research group efforts in studying and developing machine learning processes, frameworks, and knowledge base. In addition to the adaptation of the report, I add further context for general viewership comprehension.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Starting around end of May, I picked up a project pitch from my research professor. This &amp;ldquo;mini&amp;rdquo; project originally entailed benchmarking or finding a way to benchmark Machine-Learning-as-a-Service platforms. Some examples of these platforms were given: &lt;a href=&#34;https://aws.amazon.com/sagemaker/&#34;&gt;SageMaker&lt;/a&gt; by Amazon &lt;a href=&#34;https://cloud.google.com/vertex-ai&#34;&gt;Vertex AI&lt;/a&gt; by Google, &lt;a href=&#34;https://azure.microsoft.com/en-us/services/machine-learning/&#34;&gt;AzureML&lt;/a&gt; by Microsoft, &lt;a href=&#34;https://www.ibm.com/cloud/watson-studio&#34;&gt;Watson&lt;/a&gt; by IBM. With those examples in mind, I began to survey the Platform-as-a-Service scenery in search for similar services.&lt;/p&gt;
&lt;h2 id=&#34;expected-outcomes&#34;&gt;Expected Outcomes&lt;/h2&gt;
&lt;p&gt;We cannot really kick-start the research process without a certain goal in mind. Even though such goal might be vague, it still allows us to form a direction and enumerate the areas we want to look into.&lt;/p&gt;
&lt;p&gt;For this project, my immediate goal is to gather as much information about these platforms as possible and form a report. My report would include the observed characteristics of the surveyed platforms and build a comprehensive comparison system between the platforms on the features that they offer.&lt;/p&gt;
&lt;p&gt;I soon realize that this is not a small project as it now turns out to be. But the initial project startup did provoke lots of discussions and thoughts into the premise of cloud-based machine learning processes.&lt;/p&gt;
&lt;h2 id=&#34;tasks&#34;&gt;Tasks&lt;/h2&gt;
&lt;p&gt;Based on the expected outcomes, I devised the following tasks to better anchor my evaluation process and to provide a better idea on what data to gather.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shortlist the platforms to assess
&lt;ul&gt;
&lt;li&gt;Taxonomy of platforms providing AI/ML services&lt;/li&gt;
&lt;li&gt;Categorize all existing platforms&lt;/li&gt;
&lt;li&gt;Analyze CI/CD pipelines offered&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Analyze existing scientific literatures (attached in References)&lt;/li&gt;
&lt;li&gt;Choose datasets and models for benchmark&lt;/li&gt;
&lt;li&gt;Evaluate UX from data ingress, model training, inference, and interpret results&lt;/li&gt;
&lt;li&gt;(If possible) Automate model training for statistical iterations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But before getting to those tasks, we will need to understand some concepts first&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;machine-learning-operations-mlops&#34;&gt;Machine Learning Operations (MLOps)&lt;/h2&gt;
&lt;h3 id=&#34;state-of-the-union&#34;&gt;State of the Union&lt;/h3&gt;
&lt;p&gt;MLOps recently massively exploded in popularity. It is not exactly something new for the industry professionals (this is my own claim); however, for the general populace, the term experiences a huge buzz. My conjection is that the process described in MLOps literatures has always been extensively used and constantly improved by the engineers working on such project. Only now, that it is being recognized with a de facto name brings much attention. With such attention, academia and newbies in the field are pouring increased effort in researching and further pushing the boundaries of MLOps.&lt;/p&gt;
&lt;h3 id=&#34;what-is-mlops&#34;&gt;What is MLOps?&lt;/h3&gt;
&lt;h4 id=&#34;more-background&#34;&gt;MORE background&lt;/h4&gt;
&lt;p&gt;What is it exactly? To explain this, we need to go back to the basic building blocks of machine learning (ML).&lt;/p&gt;
&lt;p&gt;In layman&amp;rsquo;s terms, ML involves developing a prediction system that makes use of historical data. From a high-level standpoint, this prediction system relies on cleverly designed algorithms to take in the input data and spew out result. This result can either be some raw values that need to be aggregated first or direct predictive values that are readily actionable.&lt;/p&gt;
&lt;p&gt;$known_ data \rightarrow ML\ system \rightarrow predictive_ data $&lt;/p&gt;
&lt;p&gt;These algorithms perform pattern recognition. For example, in the case of a linearly distributed 2D dataset, the algorithm that fits the curve is linear regression (like this image with random values below).&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h4 id=&#34;machine-learning-operations&#34;&gt;Machine Learning Operations&lt;/h4&gt;
&lt;p&gt;Ok, imagine DevOps, but in the context of machine learning.&lt;/p&gt;
&lt;p&gt;On a more serious note, MLOps is more or less relying on the same principles and processes of DevOps. The lifecycle of one or many ML models from raw data to training to deployment for production use is broken down into more tangible steps and tasks to act as a banner of guidance for an engineering team when they want to put ML into real-world applications and services.&lt;/p&gt;
&lt;p&gt;While it is a thought-provoking discipline, MLOps needs &lt;a href=&#34;https://www.goodreads.com/book/show/60715378-designing-machine-learning-systems?utm_medium=api&amp;amp;utm_source=author_widget&#34;&gt;a whole book&lt;/a&gt; (maybe even several) to properly be explained, by industry professionals (like through &lt;a href=&#34;https://www.coursera.org/learn/introduction-to-machine-learning-in-production&#34;&gt;a course&lt;/a&gt;), not by an apprentice on a blog post 😜.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What does this MLOps have to do with MLaaS?&lt;/strong&gt; &amp;gt; — Well, I&amp;rsquo;m glad you ask.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;MLaaS providers heavily employ the concept of MLOps in order to build up their machine learning service on the cloud platform. We&amp;rsquo;ll get into this another blog post to be concise.&lt;/p&gt;
&lt;h2 id=&#34;research-questions&#34;&gt;Research Questions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Based on what features and metrics do we measure a platform?
&lt;ol&gt;
&lt;li&gt;And which features must be excluded? And why?&lt;/li&gt;
&lt;li&gt;What features are must-have?&lt;/li&gt;
&lt;li&gt;Is it necessary to apply weights for features? And How?&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;How can we formulate a scoring system for these features?&lt;/li&gt;
&lt;li&gt;How the requirements from the datasets can be met?&lt;/li&gt;
&lt;li&gt;How can such scoring system communicates pros and cons for the audience of this research project?&lt;/li&gt;
&lt;li&gt;How can we ensure our assessments and benchmarks themselves free from bias?&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;expected-contribution-to-the-field&#34;&gt;Expected Contribution to the field&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;To form a numerical evaluation framework to evaluate MLaaS platforms&lt;/li&gt;
&lt;li&gt;To better understand how different MLaaS platforms best fit an ML workflow and tasks&lt;/li&gt;
&lt;li&gt;To assess MLaaS platforms adaptability to different workloads&lt;/li&gt;
&lt;li&gt;To evaluate usability, learning curve, explanability, and interpretability of model training process&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;existing-mlaas-providersplatforms&#34;&gt;Existing MLaaS Providers/Platforms&lt;/h2&gt;
&lt;p&gt;My survey into MLaaS platforms yielded a non-exhaustive list of providers as detailed in the proceeding table. It is to be noted that the table does not contain &lt;strong&gt;ALL&lt;/strong&gt; of existing providers. The enumeration of currently available ones is a never-ending task as every now and then a new platform is created.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;EC2 (AWS)&lt;/td&gt;
&lt;td&gt;WhyLabs.ai&lt;/td&gt;
&lt;td&gt;Obviously&lt;/td&gt;
&lt;td&gt;Vertex AI (GCP)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Compute Engine (GCP)&lt;/td&gt;
&lt;td&gt;Hasty.ai&lt;/td&gt;
&lt;td&gt;Skyl.ai&lt;/td&gt;
&lt;td&gt;Azure ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Virtual Machines (Azure)&lt;/td&gt;
&lt;td&gt;Pega.ai&lt;/td&gt;
&lt;td&gt;MLJAr&lt;/td&gt;
&lt;td&gt;SageMaker (AWS)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linode&lt;/td&gt;
&lt;td&gt;Jasper.ai&lt;/td&gt;
&lt;td&gt;Teachable Machine&lt;/td&gt;
&lt;td&gt;Baidu ML&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DigitalOcean&lt;/td&gt;
&lt;td&gt;DataRobot.com&lt;/td&gt;
&lt;td&gt;Lobe&lt;/td&gt;
&lt;td&gt;IBM Watson&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Virtual Servers (IBM)&lt;/td&gt;
&lt;td&gt;Clarifai.com&lt;/td&gt;
&lt;td&gt;RunwayML&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Cloud Compute (Baidu)&lt;/td&gt;
&lt;td&gt;MonkeyLearn.com&lt;/td&gt;
&lt;td&gt;Lityx&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Compute Services (Oracle)&lt;/td&gt;
&lt;td&gt;Akkio.com&lt;/td&gt;
&lt;td&gt;Idiomatic&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Elastic Computing (Alibaba)&lt;/td&gt;
&lt;td&gt;Levity&lt;/td&gt;
&lt;td&gt;Reveal&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In my own reasoning, the list includes compute engine and virtual private server providers because, technically, you can build a fully managed and customized MLaaS-like platform using such infrastructure. But, after lengthy discussing with the research group, I have abandoned such idea because we want to narrow down the evaluation scope.&lt;/p&gt;
&lt;h2 id=&#34;observations&#34;&gt;Observations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Commonalities:
&lt;ul&gt;
&lt;li&gt;Code workspace integration (similar to Google Colab)&lt;/li&gt;
&lt;li&gt;Data storage&lt;/li&gt;
&lt;li&gt;Model caching and saving&lt;/li&gt;
&lt;li&gt;In the end, price is scaled with the flux of data and/or runtime&lt;/li&gt;
&lt;li&gt;Offer CI/CD pipelines&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Differences:
&lt;ul&gt;
&lt;li&gt;Pricing&lt;/li&gt;
&lt;li&gt;UI/UX&lt;/li&gt;
&lt;li&gt;Complimentary service variety&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pricing-models&#34;&gt;Pricing Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Pricing models for each platform are very dynamic and specific to the ML task. Here are 3 examples:
&lt;ul&gt;
&lt;li&gt;Vertex AI (GCP) divides into 4 categories: image data, video data, tabular data, text data. Then each category is divided into specific ops, e.g. training, deployment, batch prediction.&lt;/li&gt;
&lt;li&gt;Azure ML (Microsoft) pricing is based on the type of compute nodes, plus other infrastructural services, e.g. data warehouse, analytical insights, = container registry, etc.&lt;/li&gt;
&lt;li&gt;AWS SageMaker (Amazon) similar to Azure ML which bases its costs on infrastructure (or instances) usage.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;After lengthy discussion with the group and the professor, we have realized that the current approach would be too broad to lead to tangible and meaningful results. In order to continue, I would need to go back to the drawing board, find published and peer-reviewed literatures in the domain. Completing this should give me and the group a better idea of what is missing in the field of MLaaS.&lt;/p&gt;
&lt;p&gt;With that, we shall further discuss papers in my next post and go through some practical examples of MLOps and MLaaS implementations.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Setting up a database</title>
      <link>https://blog.aaanh.com/posts/setting-up-a-db/</link>
      <pubDate>Sat, 25 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/setting-up-a-db/</guid>
      <description>Modern database orchestration, I think?</description>
      <content>&lt;p&gt;Setting up a datbase the usual way may c&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Web Development Notes</title>
      <link>https://blog.aaanh.com/posts/web-dev-notes/</link>
      <pubDate>Sun, 19 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/web-dev-notes/</guid>
      <description>Because ML and Ops are not always done through the CLI&amp;hellip;</description>
      <content>&lt;h1 id=&#34;preface&#34;&gt;Preface&lt;/h1&gt;
&lt;p&gt;Admittedly, I have a fish brain. My memory is kind of all over the place these days. So, this serves as an easily accessible mental checkpoint for me.&lt;/p&gt;
&lt;p&gt;The basic stack I mostly use and would definitely recommend for everyone to use is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Front-end: Nextjs&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;npx create-next-app --with-tailwind
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;Style: Tailwindcss&lt;/li&gt;
&lt;li&gt;ORM: Prisma&lt;/li&gt;
&lt;li&gt;Back-end: SQL or Firestore&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Or as I discovered recently, use &lt;code&gt;npx create-t3-app@latest&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;I will improve the structure of this note page bits by bits as I write down more content.&lt;/p&gt;
&lt;h1 id=&#34;into-the-notes&#34;&gt;Into the Notes&lt;/h1&gt;
&lt;h2 id=&#34;ios&#34;&gt;iOS&lt;/h2&gt;
&lt;h3 id=&#34;content-overflow-hidden-underneath-browser-elements-with-100vh&#34;&gt;Content overflow hidden underneath browser elements with &lt;code&gt;100vh&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;tl;dr&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;  npm i postcss-100vh-fix
&lt;/code&gt;&lt;/pre&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-js&#34; data-lang=&#34;js&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// next.config.js
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;module&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;exports&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// other stuff...
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;plugins&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;postcss-100vh-fix&amp;#39;&lt;/span&gt;],
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-js&#34; data-lang=&#34;js&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// postcss.config.js
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;module&lt;/span&gt;.&lt;span style=&#34;color:#a6e22e&#34;&gt;exports&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#a6e22e&#34;&gt;plugins&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// other stuff
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;postcss-100vh-fix&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; {}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-css&#34; data-lang=&#34;css&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;/* globals.css */&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;@&lt;span style=&#34;color:#66d9ef&#34;&gt;supports&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-webkit-touch-callout&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;none&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;body&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;height&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;-webkit-&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fill&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;available;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Web development, by default, has to target most platforms (R.I.P. IE) to bring the best experience to the user. In iOS device case, that unfortunately means that we need to polyfill for the &lt;code&gt;vh&lt;/code&gt; unit.&lt;/p&gt;
&lt;p&gt;The problem statement is as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You want your root element to span the entire height of the screen, but the height of the screen is being overlaid by the status bar and the address bar on iOS devices. So, your content will overflow the screen, activating the scrollbar and your design specifically does not want it to behave that way.&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    </item>
    
    <item>
      <title>Containers Go Brrrrr</title>
      <link>https://blog.aaanh.com/posts/containers-go-brrrrr/</link>
      <pubDate>Sat, 30 Apr 2022 11:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/containers-go-brrrrr/</guid>
      <description>Since about a week ago, containers were still an interesting technology that I just could not understand for the life of me. Its functional concepts are difficult to grasp at first but once I wrapped my head around the idea of having a runtime for applications, on which apps and services can operate. Ok&amp;hellip; sounds easy enough. Then comes the implementation, I got flabbergasted on so many different levels, with so many different aspects of creating, running, and maintaining (a) container(s).</description>
      <content>&lt;p&gt;Since about a week ago, containers were still an interesting technology that I just could not understand for the life of me. Its functional concepts are difficult to grasp at first but once I wrapped my head around the idea of having a runtime for applications, on which apps and services can operate. Ok&amp;hellip; sounds &lt;em&gt;easy&lt;/em&gt; enough. Then comes the implementation, I got flabbergasted on so many different levels, with so many different aspects of creating, running, and maintaining (a) container(s).&lt;/p&gt;
&lt;h2 id=&#34;what-i-have-accomplished&#34;&gt;What I Have Accomplished&lt;/h2&gt;
&lt;p&gt;So once you have learned a new technology or skill, you gotta have to put it in practice to hone and understand the scope of the tech or skill further. In this regard, I am practicing by extensively writing &lt;code&gt;Dockerfile&lt;/code&gt;s to containerize almost all, if not all, of my past and current projects. After I complete writing the configurations, I perform local build into an image, run it, test it out, and then upload the image to my private registry and deploy/publish it to the WWW.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Almost Had to RMA My Spanking New MacBook</title>
      <link>https://blog.aaanh.com/posts/almost-had-to-rma-my-macbook/</link>
      <pubDate>Sun, 10 Apr 2022 02:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/almost-had-to-rma-my-macbook/</guid>
      <description>But I managed to recover everyting in the end 😂</description>
      <content>&lt;p&gt;So&amp;hellip; the curiosity finally killed the cat. Recently, I installed Asahi Linux on my M1 Pro Macbook — my mission critical daily carry system without a flipping care in the world. It worked surprisingly well, no problems whatsoever, but I found it to be not as useful as I would have imagined. The problem didn&amp;rsquo;t lie with the engineering or stability of the Asahi Linux project, but rather it lied with the general support for Linux systems on ARM chip, especially for Arch-based distributions. Everything was fine but because of the lack of feature support, 5 hours ago, I decided to nuke the dual boot partition and reclaim the disk space, then reinstall macOS altogether because the OS was running a bit slow and hotter than usual.&lt;/p&gt;
&lt;p&gt;Without searching online beforehand, I did a full wipe of whatever partitions there were on the SSD. And that right there was the gateway drug down the rabbit hole of whatever the heck Apple was doing to configure their filesystem and disk management, with the sparkling cameos of useless answers on the Apple discussion forums by wannabe gurus and quantum pseudo tech literate self-acclaimed know-it-alls.&lt;/p&gt;
&lt;p&gt;The internal SSD of my MacBook before I solved the shenanigan, caused by blindly nuking the partitions is as followed: - disk0: 500GB — this is the physical disk total capacity - disk0s1: 500MB — GUID Partition Table - Free Space: 4xxGB — the weird-ass void - disk0s2: 2GB — the recovery partition&lt;/p&gt;
&lt;p&gt;Initially, I deleted both the macOS (~400GB) and Linux partitions (~100GB) with the command from the Terminal in Recovery boot:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;diskutil eraseVolume free free disk0s#
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;I came across this erase command from an Asahi Linux stackoverflow post in which the OP also wanted to nuke the disk like myself. However, that was the end of the story on the post without the &amp;ldquo;what to do after you nuke&amp;rdquo; instructions. Well, I thought to myself, if there&amp;rsquo;s no sequel to this post, then the subsequent process should be easy enough. Aaaand, I was dead wrong.&lt;/p&gt;
&lt;p&gt;The yeet of the SSD partitions leave behind the void which just could not be detected by the MacOS installer nor the graphical tool (diskutil). To make it detectable again, I had to do the most illogical thing which is expanding disk0s1 which was apparently the &amp;ldquo;GUID Partition Table&amp;rdquo;. It does not make sense because normally this partition just stores the partition table, not the whole usable partition space. But, before I came across this solution, I thought to myself, &amp;ldquo;what the heck, last resort, last ditch effort before I need to send in the laptop for RMA anyway.&amp;rdquo; Turns out, Apple&amp;rsquo;s ways of performing seemingly regular processes are &lt;em&gt;built different&lt;/em&gt;, I guess.&lt;/p&gt;
&lt;p&gt;A note before I end this blog, I have already tried wiping ALL partitions but they are all locked by kernel (process 0). Heck, I even tried killing the kernel 🤪 process that&amp;rsquo;s using the partitions. In my defense, I was booting from a USB so killing the kernel on the internal SSD is justifiable course of action, to be honest.&lt;/p&gt;
&lt;p&gt;So after expanding that GUID partition, the installation can be continued normally without any further hiccup. I think I&amp;rsquo;ll stray away from experimental software for now 😅 as this problem took a huge chunk of my time for troubleshooting&amp;hellip;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Trying to go only MacBook and nothing else</title>
      <link>https://blog.aaanh.com/posts/trying-to-go-only-macbook/</link>
      <pubDate>Sun, 20 Mar 2022 10:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/trying-to-go-only-macbook/</guid>
      <description>Road warrior but in cafe shops.</description>
      <content>&lt;p&gt;Around the end of December 2021, I spent most of my internship salary on a MacBook Pro 16-in. (2021) with the base specs of M1 Pro 10-Core CPU, 16-Core GPU, 16GB Unified Memory, and 512GB SSD. The purchase was and probably still is the largest single spending I have ever made and I sat on that decision ever since the first M1 was released. I glad I broiled and boiled the purchase over and over because that delay finally coincided with the release of the M1 Pro line-up.&lt;/p&gt;
&lt;p&gt;Back in 2019, around 3-4 months before COVID hit worldwide pandemic status, I sheepishly decided that I should go on a journey to minimalize my tech life. I tried to achieve this by selling off my main desktop workstation to my good friend in Vancouver and reverted back to only using the MacBook Pro 13-in (Mid 2014). I thought it was all that I needed and it really was. I was able to do everything with the exception of heavy gaming on the machine. Additionally, it had a nice compact size that made bringing it everywhere I went an absolute bliss.&lt;/p&gt;
&lt;p&gt;Then, it all went haywire. After a trip visiting my relative downtown, back when I was living near the edge of civilization at the college dorm, it started to behave weirdly. The fan kicked into 100% full swing all the time, the CPU got throttled to sub-1.0GHz speed, the I/O was unresponsive, and any attempt at doing anything was practically in vain. Luckily for me, I had a spare laptop in storage, but I still need to transfer whatever data I had left on the MBP m-2014 and the experience was painful.&lt;/p&gt;
&lt;p&gt;Going through this ordeal, I felt utterly betrayed by the tool I had placed my trust on. I thought I could simply get by with just a laptop, like the good old days, and focus on my study. But damn, the laptop could not have broken down at a better time: first research project with lots of code runtime, an engineering project with Arduino that I was the programmer, and of course, COVID happened. My backup laptop worked fine and great and all, but it was a Dell Precision M6000 and it was a tanky boi. The Dell barely fitted into my backpack. So, with the little amount of savings left in my account, I painstakingly ordered a ThinkPad X1 Carbon Gen. 4 to officially replace the 2 as my daily driver. It ran most of the tasks I threw at it and the slim form factor was good with the biggest trade-off is the thermals which could throttle quite significantly under heavy load, along with the high-pitched screechy fan noise. I clenched my teeth and went on with it.&lt;/p&gt;
&lt;p&gt;Some months later into the pandemic, I got my first part-time job into my 2nd semester at the university, doing IT stuff. With some of the first paid wages, I sold both the tanky Dell and the ThinkPad to get a MacBook Pro 13-in. (mid-2019) with the maxed out specs with the exception of storage. With some more wages, I built a new PC + a server, swearing this time, I would never even think of selling them. Then, I got my first internship and with the salary, I upgraded to the now MacBook Pro 16 with M1 Pro chip.&lt;/p&gt;
&lt;p&gt;After 3 months of usage, I can say that I am really impressed by the jump in quality, experience, and performance with the new MacBook Pro. The design yells back to the tried and tested form of the pre-2016 era, the keyboard is better, the functionalities are extended compared to the predecessors. And the return of the venerable innovation MagSafe brings me great joy. The HDMI port is an absolute clutch addition. The SDXC card doesn&amp;rsquo;t really matter to me to be honest, but I now do have the chance to run amok and take photos with my DSLR&amp;rsquo;s again so it might come in clutch as well. Then, there&amp;rsquo;s the final piece of the puzzle, the creme de la creme, nice topping on top of everything: the high-impedance audio driver. It almost feels eerie as I had just broken into the lossless high fidelity audiophile community the beginning of last year and one of the most inconvenience is the need to carry around a DAC/amp in order to listen on my Ol&amp;rsquo; mate Sennies HD6XX. Well, this new laptop just eliminates the need for it, at least for a mobile scenario or a scenario where I shut off everything else and only have the MacBook open in front of me to do a grindset of pure focus and sprint, like for example, going ham on a development project, a study session, or a research. The freedom of choice that the new MacBook brings allows me to more easily attain what I set out to do, voided of distractions and setbacks.&lt;/p&gt;
&lt;p&gt;So, considering the new bells and whistles, I have decided to, once again, go MacBook only, but I am not going to sell any of my PC&amp;rsquo;s or other high-performing laptops because I think I realize that, aside of them being safety nets if anything were to happen to my MacBook (knock on wood), is those machines are perfect for setting up my lab scenarios and servers which I could use to learn new frameworks and toolings to serve my future endeavours.&lt;/p&gt;
&lt;p&gt;To close this off, there are some caveats and the biggest takeaway if someone were to consider buying the new MacBook Pro at this moment. First, the battery does not survive &lt;em&gt;that&lt;/em&gt; well when connected to external displays. In my use case, I connect 1x 4K @ 60Hz + 1x 1080P @ 60Hz (+ the built-in ProMotion @ 50-60% brightness) and I use it mostly for web development tasks: writing code (on VSCode), looking up documentations (using Chrome), hot-reloading demo (on Firefox), and the occasional simple REST api (with Postman/Insomnia) automated tests. After around 30-45 minutes of usage, I notice that it drains 5-10% at most from 100% full charge, which is a bit jarring consider the highly efficient components that it operates on. However, I do think that there are still some optimizations to be done on the 3rd party software side to decrease the energy usage. Second, the M2 Pro is going to be released sooner or later this year and considering the product cycle, it would be in one&amp;rsquo;s best interest to wait for the next iteration to be released and buy that one than going for an M1 Pro at this point in time, to simply get more out of the cumbersome one-time purchase that leaves you seething for the amount of money spent.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s it.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>My Dev Skill Improvement Roadmap for First Half of 2022</title>
      <link>https://blog.aaanh.com/posts/dev-goals-first-half-2022/</link>
      <pubDate>Sun, 06 Mar 2022 00:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/dev-goals-first-half-2022/</guid>
      <description>Lots to learn with so little time!</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;This is a work in progress&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My tentative goal is to do software engineer in DevOps and SRE. Even though I am familiar with the stacks needed according to &lt;a href=&#34;https://roadmap.sh&#34;&gt;https://roadmap.sh&lt;/a&gt; for a DevOps/SRE, I am admittedly scatterbrained and quite disorganized and impulsive when it comes to actually learning the material. I might slightly be attention deficient, who knows, probably from devouring internet contents at a breakneck speed.&lt;/p&gt;
&lt;p&gt;So, let this post be a place where I get an anchor of some sort to remind what I need to do in order to achieve my mastery in the arts of orchestration (not the musical one!).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I embed the roadmap down here so that we don&amp;rsquo;t need to go back and forth.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&amp;lt;img src=&amp;ldquo;&lt;a href=&#34;https://roadmap.sh/roadmaps/devops.png%22&#34;&gt;https://roadmap.sh/roadmaps/devops.png&amp;quot;&lt;/a&gt; width={{ width: &amp;lsquo;640px&amp;rsquo; }} /&amp;gt;&lt;/p&gt;
&lt;h2 id=&#34;owned-skills&#34;&gt;Owned skills&lt;/h2&gt;
&lt;p&gt;I shall list the skill stacks that I have already acquired:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn a programming language: (my best 3) c++, python, nodejs&lt;/li&gt;
&lt;li&gt;Understand different OS concepts
&lt;ul&gt;
&lt;li&gt;Process management&lt;/li&gt;
&lt;li&gt;Sockets&lt;/li&gt;
&lt;li&gt;POSIX basics&lt;/li&gt;
&lt;li&gt;Networking concepts&lt;/li&gt;
&lt;li&gt;I/O Management&lt;/li&gt;
&lt;li&gt;Virtualization&lt;/li&gt;
&lt;li&gt;Memory/Storage&lt;/li&gt;
&lt;li&gt;FS&lt;/li&gt;
&lt;li&gt;initd&lt;/li&gt;
&lt;li&gt;systemd&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Managing Servers
&lt;ul&gt;
&lt;li&gt;OS
&lt;ul&gt;
&lt;li&gt;Linux-based: Ubuntu, Debian, RHEL, Arch&lt;/li&gt;
&lt;li&gt;Windows&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Terminal workflow
&lt;ul&gt;
&lt;li&gt;tmux&lt;/li&gt;
&lt;li&gt;bash scripting&lt;/li&gt;
&lt;li&gt;powershell&lt;/li&gt;
&lt;li&gt;editor: vim&lt;/li&gt;
&lt;li&gt;code compilation: gcc, make, gdb&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Local System Monitoring and Process Management
&lt;ul&gt;
&lt;li&gt;ps, top, htop&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Networking
&lt;ul&gt;
&lt;li&gt;ping, nmap, tracert, ufw/iptables, dig, netstat&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Text manipulation
&lt;ul&gt;
&lt;li&gt;awk, grep, sed, sort, cat, echo, fmt, wc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Web Servers
&lt;ul&gt;
&lt;li&gt;IIS&lt;/li&gt;
&lt;li&gt;Nginx&lt;/li&gt;
&lt;li&gt;Apache2&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Networking, Security, and Protocols
&lt;ul&gt;
&lt;li&gt;Email: SMTP, IMAPS, DMARC, SPF, DKIM&lt;/li&gt;
&lt;li&gt;HTTP/S, FTP, SSL/TLS, SSH, port forwarding&lt;/li&gt;
&lt;li&gt;OSI model, DNS, BGP&lt;/li&gt;
&lt;li&gt;Whitelisting, DMZ, Firewall&lt;/li&gt;
&lt;li&gt;Reverse proxy, load balancer&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Infrastructure as Code (IaC)
&lt;ul&gt;
&lt;li&gt;Containers: Docker&lt;/li&gt;
&lt;li&gt;Configuration Management: Ansible&lt;/li&gt;
&lt;li&gt;Container Orchestration: Kubernetes&lt;/li&gt;
&lt;li&gt;Provisioning: Terraform&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Infrastructure Monitoring
&lt;ul&gt;
&lt;li&gt;Grafana, Prometheus&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CI/CD
&lt;ul&gt;
&lt;li&gt;Github Actions&lt;/li&gt;
&lt;li&gt;Azure DevOps&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logs Management
&lt;ul&gt;
&lt;li&gt;Splunk&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Providers
&lt;ul&gt;
&lt;li&gt;Type 1:
&lt;ul&gt;
&lt;li&gt;AWS&lt;/li&gt;
&lt;li&gt;Google Cloud&lt;/li&gt;
&lt;li&gt;Azure&lt;/li&gt;
&lt;li&gt;Heroku&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Type 2:
&lt;ul&gt;
&lt;li&gt;Linode&lt;/li&gt;
&lt;li&gt;Digital Ocean&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;skills-need-to-learn-and-improve&#34;&gt;Skills need to learn and improve&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;OS Concepts:
&lt;ul&gt;
&lt;li&gt;Threads and Concurrency&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;System Performance:
&lt;ul&gt;
&lt;li&gt;all&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Web technologies:
&lt;ul&gt;
&lt;li&gt;WebSockets&lt;/li&gt;
&lt;li&gt;REST api, CRUD&lt;/li&gt;
&lt;li&gt;Webhooks&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;IaC:
&lt;ul&gt;
&lt;li&gt;Docker (more advanced)&lt;/li&gt;
&lt;li&gt;Ansible (playbook to galaxy)&lt;/li&gt;
&lt;li&gt;Kubernetes (still beginner)&lt;/li&gt;
&lt;li&gt;Terraform (still beginner)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Networking and Security:
&lt;ul&gt;
&lt;li&gt;Reverse proxy (more advanced configs)&lt;/li&gt;
&lt;li&gt;Load balancer (still beginner)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logs management:
&lt;ul&gt;
&lt;li&gt;Splunk (more!)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;CI/CD:
&lt;ul&gt;
&lt;li&gt;Azure DevOps&lt;/li&gt;
&lt;li&gt;Circle CI&lt;/li&gt;
&lt;li&gt;Gitlab CI&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cloud Providers:
&lt;ul&gt;
&lt;li&gt;AWS (backlog)&lt;/li&gt;
&lt;li&gt;Google Cloud (learning Cloud Engineer associate)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I will need to try and complete at least the Google Cloud Engineer associate learning path, reinforce my knowledge on IaC tools and frameworks, learn more about Azure DevOps and Circle CI, and then finally complete a course on REST api framework (nestjs) and kubernetes.&lt;/p&gt;
&lt;p&gt;My goals seem a bit outlandish and probably unachievable and I will probably fail, but at the very least, I have a good image of what I need to do (or so it seems).&lt;/p&gt;
&lt;p&gt;On another note, I am tasked with a benchmark project for MLaaS providers by my research supervisor and so far, looks like it will play well into the whole shenanigans so I will have the chance to improve hollistically. We&amp;rsquo;ll see.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>First Code Competition</title>
      <link>https://blog.aaanh.com/posts/productivity-and-operations/</link>
      <pubDate>Thu, 24 Feb 2022 19:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/productivity-and-operations/</guid>
      <description>This post is going to be written in the style of an incident postmortem. :^)
Overview Those who are in the dev scene probably know, to some extents, what competitive programming is. It is basically Leet Code on the highest purity crack. Each problem presented to the participants during the competition can said to be comprised of multiple Leet Code problems in the topics of algorithm, data structure, optimization, dynamic programming, so on and so forth.</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;This post is going to be written in the style of an incident postmortem. &lt;code&gt;:^)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Those who are in the dev scene probably know, to some extents, what competitive programming is. It is basically Leet Code on the highest purity crack. Each problem presented to the participants during the competition can said to be comprised of multiple Leet Code problems in the topics of algorithm, data structure, optimization, dynamic programming, so on and so forth.&lt;/p&gt;
&lt;p&gt;Today, it was my first time participating in such a competition, virtually. It was Hash Code and organized by Google. I am proud to say that our team of three did not complete the challenge. But it was never about the end result, it always has been the friends we made along the way &lt;code&gt;:^)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;]The qualifier round is truly fitting of being a Google competition. Details of which can be read here &lt;a href=&#34;https://codingcompetitions.withgoogle.com/hashcode&#34;&gt;https://codingcompetitions.withgoogle.com/hashcode&lt;/a&gt;. Not gonna lie, it was &lt;em&gt;really&lt;/em&gt; hard. The competition presents a multi-faceted problem with a lot of aspects the solvers need to account for. And according to the Google engineer who organizes this year&amp;rsquo;s HashCode in the post-round livestream, there is yet to be an optimal solution.&lt;/p&gt;
&lt;h2 id=&#34;timeline&#34;&gt;Timeline&lt;/h2&gt;
&lt;p&gt;Back to our team&amp;rsquo;s performance during the qualifier round. Here is our timeline of the event with our progress. Note that all the timestamps are documented on &lt;strong&gt;2022-02-24&lt;/strong&gt; and according to the &lt;strong&gt;EST&lt;/strong&gt; timezone.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;11:45AM&lt;/code&gt; I reserved a conference room for our physical collab space.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;12:00PM&lt;/code&gt; Initial equipment and room setup completed.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;12:30PM&lt;/code&gt; Hash Code 2022 Qualifier round started and I started problem comprehension while waiting for the rest of the team to arrive.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;12:45PM&lt;/code&gt; The team was assembled and we debriefed the problem statement.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;01:30PM&lt;/code&gt; Whiteboarded the data structure and started implementing while figuring out the problem goal.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;02:00PM&lt;/code&gt; Partially completed input parsing code. Tested against example input and realized we overshot the line traversal.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;02:30PM&lt;/code&gt; Completed input parsing code. Tested against all input sizes and types. Test completed without errors.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;02:45PM&lt;/code&gt; Analyzing aspects of the problem: selecting important features, extracting and interpolating metrics, and building decision tree system.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;03:30PM&lt;/code&gt; Analysis still in progress but a rough idea of how we should tackle the scheduling and optimizing was realized.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;03:45PM&lt;/code&gt; Competition ended. We stayed for a bit to further explore edge cases and watched the post-round livestream to understand more of the real-life aspects and metadata of the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;action-items&#34;&gt;Action Items&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;#&lt;/th&gt;
&lt;th&gt;Action Item&lt;/th&gt;
&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Status&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Problem comprehension&lt;/td&gt;
&lt;td&gt;Holistic&lt;/td&gt;
&lt;td&gt;Partial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;Data Structure: OOP&lt;/td&gt;
&lt;td&gt;Input&lt;/td&gt;
&lt;td&gt;Done&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Data Parsing&lt;/td&gt;
&lt;td&gt;Input&lt;/td&gt;
&lt;td&gt;Done&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;Optimize selection&lt;/td&gt;
&lt;td&gt;Logic&lt;/td&gt;
&lt;td&gt;Partial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;Optimize project order&lt;/td&gt;
&lt;td&gt;Logic&lt;/td&gt;
&lt;td&gt;Partial&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;Incorporate skills into optimization&lt;/td&gt;
&lt;td&gt;Logic&lt;/td&gt;
&lt;td&gt;Not started&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;analysis&#34;&gt;Analysis&lt;/h2&gt;
&lt;h3 id=&#34;expectations&#34;&gt;Expectations&lt;/h3&gt;
&lt;p&gt;Upon reading the problem and estimating the complexity, we expected to at least be able to run the first example and output the first output correctly.&lt;/p&gt;
&lt;h3 id=&#34;result&#34;&gt;Result&lt;/h3&gt;
&lt;p&gt;We was not able to meet the expectations. But we managed to develop our ideas for tackling and optimizing the problem. Our tangible result was the developed data structure and input parsing logic.&lt;/p&gt;
&lt;h3 id=&#34;my-performance&#34;&gt;My performance&lt;/h3&gt;
&lt;p&gt;The majority of the analysis will be on my own performance as I cannot speak for others.&lt;/p&gt;
&lt;p&gt;I am the weakest link in the team due to my lesser experience in the Leet Code scene and the competitive programming scene as compared to the other team members. My input to the comprehension progress needs further polishing in terminologies and presentation, as well as the speed at which I can deconstruct and organize various aspects of the coding challenge. The main contribution I made weighed in in terms of writing code and implementing the ideas we had developed so far at the time. Although, on that note, I would still need to improve my coding effiency and debugging skills.&lt;/p&gt;
&lt;h3 id=&#34;what-i-learned-and-remediations-going-forward&#34;&gt;What I learned and remediations going forward&lt;/h3&gt;
&lt;p&gt;Experience would really come in handy. This is explicitly apparent when I see the winners&amp;rsquo; names are mostly from past competitions. I would need to solve more algorithmic problems to train myself to be come better at that way of thinking. Along with that process, I would also be able to improve my programming efficiency.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Cloudflare FTW</title>
      <link>https://blog.aaanh.com/posts/cloudflare-ftw/</link>
      <pubDate>Mon, 31 Jan 2022 03:00:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/cloudflare-ftw/</guid>
      <description>My whole networking infrastructure is hosted on Cloudflare.</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Totally not paid to say this. I shill what I (ab)use.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;before&#34;&gt;Before&lt;/h1&gt;
&lt;p&gt;I have been abusing Cloudflare (NYSE: NET) ever since the day I discovered Cloudflare, which is probably 1-2 year(s) ago. I am probably one of the living testaments to why Cloudflare stock 🚀 to the 🌙 in the one-year period. Aside from servers I have physically at home, there are also my distributed infrastructures on various cloud platforms, PaaS like Vercel and Netlify to IaaS like Linode and Compute Engine/EC2/Azure.&lt;/p&gt;
&lt;p&gt;I initially came to Cloudflare for its Domain Name Service back in 2018. Before transferring everything to Cloudflare, the routing scenery was really bleak with pay-as-you-go service providers like routing in Googel Cloud or the Route 52 routing on AWS or from the registrar or from the hosting service. Although those providers offerred either an quick and easy way to route or a granulated control over all the settings, the feature sets were either too simpleton or too complicated or the interface was too cluttered and cumbersome (👀 at you NASDAQ:AMZN).&lt;/p&gt;
&lt;h1 id=&#34;after&#34;&gt;After&lt;/h1&gt;
&lt;p&gt;Initially, the only service that cloudflare provides that I make use of is the DNS. Compared to other providers, Cloudflare, hereforth referred to as &lt;strong&gt;CF&lt;/strong&gt;, provides a more centralized way of managing DNS with a UI that looks like it was actually minted in the modern day with material design. Got a server running on Linode? Route its IP to CF and assign a subdomain of my choice. Got a web application (Nextjs) running on Vercel? Simply route the Vercel nameservers to CF with the proper (sub)domain names. For SSL, most modern PaaS provides the SSL configurations so I only have to flip a switch on CF to enable the HTTPS enforcement. In terms of access, I can create rules for redirecting or blocking access on CF&amp;rsquo;s side instead of doing it on the host configurations, which are prone to misconfiguration and creating security risks.&lt;/p&gt;
&lt;p&gt;Nowadays, CF has a madness amount of services available for free on its platform. I have used a bunch of which and am liking them so far in terms of usability and ease of setup. Especially,one of which I recently discovered and tried with success is the Cloudflare Access.&lt;/p&gt;
&lt;h1 id=&#34;present-day&#34;&gt;Present Day&lt;/h1&gt;
&lt;p&gt;Cloudflare Access is part of CF&amp;rsquo;s newly minted Zero Trust Platform — the favorite poster child of a buzzword in the tech scene. During last month (December) seasonal break, I got into discovering CF Access to see what could be &lt;del&gt;abused&lt;/del&gt; leveraged from the spanking new platform/module. To simply describe, CF Access, more hollistically referred to as &amp;lsquo;Zero Trust&amp;rsquo;, is MSFT Azure in a more infantile and watered down form. This means that it is easier to understand what&amp;rsquo;s going on with the platform and I don&amp;rsquo;t get overwhelmed with the amount of information/services it provides, which ultimately leads to my ability to configure CF Access for use. And have I mentioned that it&amp;rsquo;s &lt;strong&gt;free&lt;/strong&gt;? Bet neither Azure nor Google Cloud could do that.&lt;/p&gt;
&lt;p&gt;Who is CFA geared towards? I would say that it&amp;rsquo;s more for small to medium enterprises that don&amp;rsquo;t necessarily require complex access control and networking systems or simply don&amp;rsquo;t have the manpower and the financial means to granually configure a IAM, networking, and directory system from the ground up. The ability to quickly configure these services, albeit at the price of platform immaturity, is even more amplified when the distributive nature of the workplace during and after the pandemic comes into play.&lt;/p&gt;
&lt;h1 id=&#34;my-abuse&#34;&gt;My (Ab)use&lt;/h1&gt;
&lt;p&gt;I have always been enthralled by the idea of having access to my workstation from anywhere. Since my workstation is Windows, the logical quick and easy approach to this would be using the built-in Remote Desktop Protocol routed through a domain name pointing straight at my home IP, assigned by the ISP. This was accomplished with just Cloudflare DNS and port forwarding on the home router. However, this approach is dangerously vulnerable to attacks because 1. The connection cannot proxied through CF DNS and thus exposed to the interweb, 2. Leaking the IP address would create a healthy attack surface for all of my home devices, including my homelab cluster. In other words, it&amp;rsquo;s bad.&lt;/p&gt;
&lt;p&gt;But, that was all I could do. An RDS server would be nice that MSFT is stingy so that costs a fortune for the license. I don&amp;rsquo;t want to do VNC because that means having an extra service running on the background and at startup. I probably could tolerate unproxied connection to the workstation plus implement 2FA when connecting and SSL to strengthen the security? Welp, does not work on personal stuff because those are offerred by the RDS server.&lt;/p&gt;
&lt;p&gt;Last night, I was perusing through CF Access documentation and saw there is an article on how to use a CLI called &lt;a href=&#34;https://github.com/cloudflare/cloudflared&#34;&gt;cloudflared&lt;/a&gt; to set up a Cloudflare Tunnel. Digging deeper, I found out that it supports RDP tunneling and comes with a documentation on how to do it. Jackpot: &lt;a href=&#34;https://developers.cloudflare.com/cloudflare-one/tutorials/rdp&#34;&gt;https://developers.cloudflare.com/cloudflare-one/tutorials/rdp&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The way it works is that Cloudflared is run on both the RDP host and the client. This creates a private tunnel with specific ports between the two machines. The host first needs to create an application connection and add ingress for the connection along with the secret key file (generated during the creation process) in the local &lt;code&gt;config.yaml&lt;/code&gt; file. The cloudflared CLI can also create a DNS record specific for this RDP routing. Then, the client can start the tunnel, routing traffic from the RDP port from localhost to target host via the (sub)domain name and the app name. Once the connection to the app on Cloudflare Access is established, the client can start the RDP session.&lt;/p&gt;
&lt;p&gt;The extremely cool thing about this setup is that RDP connection must be authenticated through CF Access and since I have added several oauth authentication strategies on the admin panel, I could either sign in on the browser any strategy, e.g. Github, Google, etc. Once I&amp;rsquo;m signed in, the authentication is verified and the RDP session can start.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a diagram I cooked up in 5 minutes to illustrate what happens.&lt;/p&gt;
&lt;p&gt;&amp;lt;img
src=&amp;ldquo;&lt;a href=&#34;https://lh3.googleusercontent.com/pw/AM-JKLW_Sw3s-3yNsSWKyMUg0WGRHSLLgGwunEh3HqADw2ki425HQJC8cbeLSkdYw4bfvMNBPMDBUA8QAWgkRAFyeMCPV9ErmY2jM2rBgJjJzcKsz93fWun2c6dWQhrlTgWwTy_dta9oq-1xFJxFh2bdrGsOVw=w2198-h1230-no?authuser=0%22&#34;&gt;https://lh3.googleusercontent.com/pw/AM-JKLW_Sw3s-3yNsSWKyMUg0WGRHSLLgGwunEh3HqADw2ki425HQJC8cbeLSkdYw4bfvMNBPMDBUA8QAWgkRAFyeMCPV9ErmY2jM2rBgJjJzcKsz93fWun2c6dWQhrlTgWwTy_dta9oq-1xFJxFh2bdrGsOVw=w2198-h1230-no?authuser=0&amp;quot;&lt;/a&gt;
style={{ width: &amp;lsquo;640px&amp;rsquo; }}
/&amp;gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>I like Vim now</title>
      <link>https://blog.aaanh.com/posts/i-like-vim-now/</link>
      <pubDate>Thu, 20 Jan 2022 20:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/i-like-vim-now/</guid>
      <description>Choose whatever works best for you. No need to debate.</description>
      <content>&lt;p&gt;That. Is. It. After several months of getting used to using &lt;code&gt;vim&lt;/code&gt;, I think I can properly like it now. At least for CLI-based workflows, I absolutely adore it. Although, for GUI workflows, like using on a Linux desktop, or Mac, or Windows, the traditional code editor is still way more convenient and suits the user interaction flow better. I mean, why would the desktop applications exist in the first place and thrive to contemporary popularity if it is not necessary.&lt;/p&gt;
&lt;p&gt;That being said, I admit that I am not utilizing Vim to its full potential yet, since the navigation and functions are based on sometimes very arbitrary shortcut keys and my brain is comparable to that of a goldfish who has failed in life. For example, going forward by word is &lt;!-- raw HTML omitted --&gt;w&lt;!-- raw HTML omitted --&gt; and my FPS brain is conditioned to &lt;!-- raw HTML omitted --&gt;s&lt;!-- raw HTML omitted --&gt; as backward by word. But, it&amp;rsquo;s actually not even a letter-based key, it is actually &lt;!-- raw HTML omitted --&gt;Backspace&lt;!-- raw HTML omitted --&gt; (or &lt;!-- raw HTML omitted --&gt;delete&lt;!-- raw HTML omitted --&gt; on a Mac). If I were someone who recognized the hidden power of Vim trying to learn Vim, I would probably not change the shortcut bindings the first thing when I dive in.&lt;/p&gt;
&lt;p&gt;There are quirks to using Vim and a steep learning curve for sure, but I absolutely am loving the ability to &lt;strong&gt;NOT&lt;/strong&gt; having to move my hand out of the way to use the mouse pointer nor do I have the ability to do in the first place with a CLI. Currently, my hurdle and damnation that is preventing me from not fully embracing Vim as my go-to editor is probably the convenience of Visual Studio Code on a GUI system. Whilst VS Code has the Vim extension to emulate for editing, the Vim shortcuts sometimes get conflicted with those of VS Code. I might be able to avoid this by simply using Vim from the Terminal shell but there are integrations and other extensions on VS Code that are, more or less, quite crucial to my productive workflow.&lt;/p&gt;
&lt;p&gt;That takes us to the extension realm. Vim, no doubt, has an amazing extension and plugin selection out there, online, for anyone to grab and install and ultimately personalize their Vim editor. When talking about this aspect, it is mostly for the Terminal-based use case, as VS Code or other graphical editors have their own repositories of extensions and plugins. Back to the point, installing Vim extensions, or more accurately referred to as &amp;lsquo;plugins&amp;rsquo;, can be an arduous undertaking. There are several plugin managers for Vim and from which I need to choose one to manage the plugins I would want to install in the future. Among those, a plugin I need might be in one but not the other plugin manager.&lt;/p&gt;
&lt;p&gt;Overall, I digress. From an objective view, Vim is a significant productivity tool with the main gotcha is that one actually needs to discard one&amp;rsquo;s preconceived and/or preconditioned behaviors and learn a whole new skillset. Then, Vim would become the greatest bazooka to shoot through whatever editing needs. I mean, if it is not &lt;em&gt;that&lt;/em&gt; good as everyone says it is, why would it even continue to exist in this day and age?&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Security or Convenience</title>
      <link>https://blog.aaanh.com/posts/security-or-convenience/</link>
      <pubDate>Tue, 18 Jan 2022 20:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/security-or-convenience/</guid>
      <description>Can&amp;rsquo;t be too secured or else I&amp;rsquo;d be better off in a prison cell.</description>
      <content>&lt;h2 id=&#34;preface&#34;&gt;Preface&lt;/h2&gt;
&lt;p&gt;After my internship in the Fall (as in Autumn) of 2021, I have drastically become more &lt;del&gt;anal&lt;/del&gt; diligent about securing my digital interactions. Ignorance had been a bliss up till then, but now that I am enlightened about the threats looming in the cyberspace I will forever be consumed by the urge to secure every aspect of my footprints on the interweb.&lt;/p&gt;
&lt;h2 id=&#34;state-of-the-union&#34;&gt;State of the union&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: I won&amp;rsquo;t be citing or quoting sources because these facts are either intuitively observed, or they are generally known and can be looked up easily.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With the world moving online for the pandemic to continue their study, work, and entertainment, the rise in usage of internet-based services and information exchange platform generates goldmines that are ripened for the taking by cybercriminal organizations. According to my very trusted sources, Reddit and maybe the CVE feeds, I have observed that almost every day there are new cool exploits discovered here and there in various systems, be it enterprise or hobby open-source or your mom&amp;rsquo;s newly purchased smart doorbell and toaster oven. Among those are critical 0-day vulnerabilities that could be on probably more than 3 billion devices, leaving them stripped naked, taking on full-frontal assaults from the wild and rough attacks from threat actors (I&amp;rsquo;ve been watching too much penguinz0).&lt;/p&gt;
&lt;p&gt;Aside from the inadequately secured and monitored systems, the userland is not looking any less bleaker. Untrained or unaware users with the art of cyber-war from Sun Tzu, the hackerman from 544 BC (birth-year-src=wikipedia), are primary targets for social engineering attacks like phishing, MITM, or straight up brute-forcing the credentials because &amp;lsquo;password123&amp;rsquo; is so easy to type and remember.&lt;/p&gt;
&lt;p&gt;For me personally, I have my physical and digital information ~literally littered~ everywhere, except the TikTok cesspool. So, it is my due diligence to secure my data online. In terms of attack surfaces, I have from the very basics everyday-Joe like emails and (social) accounts to mad-lad infrastructures like private emails, compute servers, and DNS are basically lying there waiting to get hacked by whatever methods.&lt;/p&gt;
&lt;h2 id=&#34;methods-of-securing&#34;&gt;Methods of Securing&lt;/h2&gt;
&lt;p&gt;First topic, account passwords are practically deprecating technology since they are not dynamic, crackable (albeit might take time), and cumbersome (for each account in 100_accounts: set new_password). Although there are pushes from the industry for passwordless authentication, based on what the currently available technologies, such authentication method does not bode really well since the integration and implementation progress is barely catching up. So, password is here to stay and quite frankly unavoidable. I have no choice but to secure the passwords as well. From my understanding, there are just 2 main ways to secure the ye olde password: complex and long strings, failsafe with Multi-Factor Authentication. I don&amp;rsquo;t include solutions like password managers because they still need a complex master password and an authentication code which are basically the same thing. I also lump using key-pairs into the password diaspora since they could easily be stolen as a (crackable) binary/ASCII/etc. format. So, I use non-dictionary strings with complex characters as password while retroactively changing old passwords up to my current personal standards. In addition, I enable MFA on every account where I could. As crazy as it sounds, there are still systems without MFA capabilities or systems with MFA loopholes; then, the only line of defense is from the character strings. Regarding setting up MFA, there are still many improvements to be made with the authentication process to make it more convenient and thus more prevalent among end-users. Putting on a phone is a good convenience. I also recently tried Smart Card PIV, using a yubikey, to generate auth tokens on desktops which a tad bit more convenient because I don&amp;rsquo;t have to pick up an external device that would break away from that contemporary context (context switching is really expensive!).&lt;/p&gt;
&lt;p&gt;Second topic, hardware and servers need to be secured as well. If they were to be accessed and used by attackers, that action would incur infrastructural and maintenance costs and might as well sully my identity and create distrust among whomever I am in communication with. Of course, managing access to those systems are already secured by the aforementioned auth methods, but there could be loopholes and exploits that simply bypass the need for authentication. The situation surrounding these systems is always evolving: this vulnerability is patched, another vulnerability pops up. To achieve a perceived sense of a secured system, I keep my systems as up-to-date as possible without completely breaking stuff down in production. I employ SSL on exposed protocols like http(s), rdp/rds, key-pair and fingerprinting for ssh connections, etc. I set up firewall for all systems and filter IPs as well as monitoring through a tool for suspicious activities.&lt;/p&gt;
&lt;p&gt;Third topic, it is important to obfuscate paths to whatever resources or systems that I deploy and expose online. For example, I am assigned a private IP for my home internet that I can forward local services and resources like remote desktop or ssh hosted on my stationary devices (physical servers). Now, this IP address can be used to point straight at my face from anywhere which would introduce a security risk to my personal information and private data should the threat actors gain access to my home network through whatever exploits there are. So, to obfuscate it, I route through a DNS provider with the ability to proxy access, filter IPs, block DDoS, fight bots, and apply edge routing and accessing rules. I prefer this configuration because I am not bothered with the advanced details (which instead I will probably equip myself in the future) and the cross-operations between security features, which might not be configured correctly, incur technical debts, and grow into a pain in the ass to improve and maintain.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;That is all for my strategies for staying sane in this world.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Automated Development Pipeline</title>
      <link>https://blog.aaanh.com/posts/automated-dev-pipeline/</link>
      <pubDate>Tue, 28 Dec 2021 20:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/automated-dev-pipeline/</guid>
      <description>Allows room for periodical laziness.</description>
      <content>&lt;p&gt;I have successfully taken the first steps towards automating my development process. The automated process is applied to 2 projects, 1 of which is my own unix wiki publicly available at &lt;a href=&#34;https://githuh.com/aaanh/linux-docs&#34;&gt;https://githuh.com/aaanh/linux-docs&lt;/a&gt; with its homepage at &lt;a href=&#34;https://unix.hoanganh.tech&#34;&gt;https://unix.hoanganh.tech&lt;/a&gt;, while the other one is a private wiki project and therefore not exposed to public network.&lt;/p&gt;
&lt;p&gt;The primitive automation relies on Github Actions. Once a new commit or a feature branch is merged into the production branch, Github Actions will be triggered and it will spin up a build environment that is specified in the &lt;code&gt;./github/workflows/main.yml&lt;/code&gt; file. I will talk only about the process for the public wiki project but the private project follows somewhat the same principals.&lt;/p&gt;
&lt;p&gt;The unix wiki is hosted on a Ubuntu server on Linode, so the first hurdle to overcome is how to &amp;ldquo;sync&amp;rdquo; new changes to the server with just the event of pushing new commits to the production branch. Luckily for me, features and functions within this space of technology have developed enough to let me have my ways without much hassle.&lt;/p&gt;
&lt;p&gt;The wiki site is built using the sphinx-doc tool that can generate static webpages completed with table of contents and hyperlinks from markdown sources, while incorporating theming and other features using &lt;code&gt;pip&lt;/code&gt; modules. So, the process would look something like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-txt&#34; data-lang=&#34;txt&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;local changes &amp;gt; push to origin &amp;gt; github actions voodoo to build &amp;gt; &amp;#34;sync&amp;#34; build results to /var/www/... on the Ubuntu server.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Github Actions shares similarity with docker in terms of configurations so writing a config file for the build process is not that difficult. Github Actions is actually the 3rd option that I tried, after Cloudflare Pages and Vercel. For some reasons, Pages and Vercel build configs and environment (presumedly, debian-based) do not have what is needed to build sphinxd-docs. In addition, the restrictive nature and non-granularity of the configs prevents me to properly debug the failed builds. Therefore, I finally land myself at Github Actions (hereforth referred to as Actions).&lt;/p&gt;
&lt;p&gt;As said before, Actions functions similar to a docker container. Therefore, I need to specify what image to load and the Ubuntu image works perfectly because it features an environment and a package manager that I am confident with. Below is the content of the &lt;code&gt;.yml&lt;/code&gt; file for better visualization:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yml&#34; data-lang=&#34;yml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This is a basic workflow to help you get started with Actions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;CI&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Controls when the workflow will run&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;on&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# Triggers the workflow on push or pull request events but only for the master branch&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;push&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;branches&lt;/span&gt;: [&lt;span style=&#34;color:#ae81ff&#34;&gt;prod]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# Allows you to run this workflow manually from the Actions tab&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;workflow_dispatch&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# A workflow run is made up of one or more jobs that can run sequentially or in parallel&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;jobs&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# This workflow contains a single job called &amp;#34;build&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;build&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# The type of runner that the job will run on&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;runs-on&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ubuntu-latest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;# Steps represent a sequence of tasks that will be executed as part of the job&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;steps&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Install SSH Key&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;uses&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;shimataro/ssh-key-action@v2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;with&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;key&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;${{ secrets.LINUXHOANGANHTECH }}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;known_hosts&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;placeholder&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Adding server to known_hosts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ssh-keyscan -H ${{ secrets.SSH_HOST }} &amp;gt;&amp;gt; ~/.ssh/known_hosts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Checkout repo code&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;uses&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;actions/checkout@v2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Install Sphinx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sudo apt install python3-sphinx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Install pip&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;sudo apt install python3-pip&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Install pip dependencies&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;python3 -m pip install -r requirements.txt&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Generate build with sphinx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;make html&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Rsync to server&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;run&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;rsync -avz ./build/ ${{ secrets.SSH_USER }}@${{ secrets.SSH_HOST }}:/var/www/html/build/&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The build steps are easily constructed as a line by line bash script and each steps can be further described with a name and that name is echoed during the build for better debugging insights. Here is a run through of the how the process works:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Pre-configs: build environment secrets are added to Actions: ssh-key, IP address, pathing on the server, and other credentials.&lt;/li&gt;
&lt;li&gt;The seceret SSH key is installed by using an action I found on the marketplace. This key is used to, you&amp;rsquo;ve guessed it, establish an SSH connection with the host server.&lt;/li&gt;
&lt;li&gt;The repo code is then checked out.
&lt;ul&gt;
&lt;li&gt;Note that it is not the same as cloning the repo into the build environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The build dependencies are installed through &lt;code&gt;apt&lt;/code&gt; and &lt;code&gt;pip&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;make&lt;/code&gt; command is run to generate the static site files.&lt;/li&gt;
&lt;li&gt;Rsync is used to transfer the build files to the host server directly through the &lt;code&gt;ssh&lt;/code&gt; connection to its public IP address on Linode.
&lt;ul&gt;
&lt;li&gt;Note that this public IP is also added as environment secret and site is actually proxied through Cloudflare.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;And that is the end of the process. Originally, I have wished for it to be this way for a long time already but only now do I have the capacity, knowledge and time, to finally set up the very basic pipeline.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>I&#39;m officially done with Windows 11</title>
      <link>https://blog.aaanh.com/posts/windows-11-beta-postmortum/</link>
      <pubDate>Mon, 06 Sep 2021 23:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/windows-11-beta-postmortum/</guid>
      <description>Oh god, please no, stop.</description>
      <content>&lt;p&gt;After more than a month of usage, I have now wiped my Windows 11 boot drive and reinstall a fresh copy of Windows 10. Here is my journey.&lt;/p&gt;
&lt;p&gt;Back in the beginning of August when people were freaking out about the purportedly leaked Windows 11 super secret alpha/beta, I managed to get into the Windows Insider program, configured my desktop to match the specs requirements (TPM and secure boot), and upgraded to Windows 11. My first impressions of Windows 11 are mostly positive: it has nice eye-candies, revamped system UI and dialogs, and a genuinely smooth and fast experience without too many bugs.&lt;/p&gt;
&lt;p&gt;Then, I opened a can of worms&amp;hellip; Windows 11 is littered with anti-consumer tactics designed to lock the user into the shitty first-party software offerings from Microsoft. I am looking at Edge. Although Edge has gone Chromium for a while and might be even better than Chrome in terms of performance. I&#39;m giving quite a big benefit of the doubt here. However, that is not my qualm with Edge, it is the hinderance of free choice, or plainly lack thereof. The evil big corporation has struck end-users once again. Changing default programs is now done at the protocol/file type granularity, one is no longer able to change the default programs for a set of functions (e.g. web browsing, music player, etc.) with just one click (or 3-4 clicks in the Windows 10 case).&lt;/p&gt;
&lt;p&gt;I might be cherry-picking this particular fault. But after using Windows 11 extensively for various tasks and workflows, the default program was the tipping point. Prior to that, there had been already many stupid UX decisions. Perhaps one of the more noticeably annoying change is the simplification of the right-click context menu. Want to extract with 7z? At least 3 clicks. Want to do anything that is not copy-paste or whatever the basic functions are? At least 2 clicks, and I am being generous here. Other than requiring more energy and time to perform an action, such design inhibits one&amp;rsquo;s performance in their workflow, (in)effectively increases the time during which the user is being yanked out of the thought context in order to extraneous auxiliary steps.&lt;/p&gt;
&lt;p&gt;Last but not least, the Settings app. God why? I&amp;rsquo;ll now have to use God Mode, launch .cpl and .msc via Win + R, GPO, and regedit to modify configurations that are now completely hidden out of plain sight.&lt;/p&gt;
&lt;p&gt;Conclusion, while Windows 11 mgiht be a cute little face-lift for the now aging Windows 10, it also robs users of their productivity by oversimplifying and overcomplicating certain aspects the interface into a dumbed-down fiasco of rigid proprietary piece of software gore to make their shareholders happy campers. At this rate, when the number of users starts to dwindle because of the illogical business decisions, will they able to keep the shareholders which they hold so up high on the greedy pedestal?&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Self-hosted: Go big or go... home?</title>
      <link>https://blog.aaanh.com/posts/selfhosted/</link>
      <pubDate>Wed, 25 Aug 2021 14:30:00 +0000</pubDate>
      
      <guid>https://blog.aaanh.com/posts/selfhosted/</guid>
      <description>Cloudy with a chance of hosting</description>
      <content>&lt;h2 id=&#34;preface&#34;&gt;Preface&lt;/h2&gt;
&lt;p&gt;Deployment is like a drug. Once you get that first taste of starting up a server and expose it to the interweb where you could practically access it anywhere, you will never be able to stop. I know that I have grown addicted to all this ever since my first deployment on AWS EC2 (which drained my pocket dry for some months). I had to shut down my websites and web services for a year or so. Then, the new age of the interweb finally reached and enlightened me. The spawn of free web application deployment and hosting services, like Gatsby and Vercel, and the shining jewel of affordable webhosting Linode had once again lit a fire under my butt.&lt;/p&gt;
&lt;h2 id=&#34;setting-up-a-code-server&#34;&gt;Setting up a code server&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://code.hoanganh.tech&#34;&gt;Code Server (Password-protected)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code server, in a nutshell, is a visual studio code environment that is hosted on a remote server and accessed through client server. The code server is installed on a Linux-based OS with the Linux shell exposed so that many of the software development processes can be run natively. Although there are some catches when using a custom domain name like I do, such as front-end web frameworks and backend webapp servers that utilizes the localhost:&amp;lt;port&amp;gt; to serve the development build requires additional setups. And I skip the hassle of setting up the correct port forwardings because I only use it for code editing anyway, while the deployment preview is handled by external services that pull and from github pushes and commits.&lt;/p&gt;
&lt;p&gt;The best use case of having a code server is that you can basically (edit) code anywhere without going through the strenuous process of setting up a local development environment on whatever temporary platform you use.&lt;/p&gt;
&lt;p&gt;See the code server running on Chrome below:&lt;/p&gt;
&lt;p&gt;&amp;lt;img
src=&amp;ldquo;&lt;a href=&#34;https://lh3.googleusercontent.com/AdEHpUUTC1BK7Smsg4jAi3cEadKR6qtvv0GMxtVRui0L83viwc7A6NlZveuzMJI2Hd5d_oBX4gRwMg0_BzbjQaCOa4CbznC8KXe9LiqE4bq4uFZ9OwQsc0dbAPmjCbrxBTOiCxVQ84e8HeqMdkuzqE2IQjImKGnOq0wL4oxwnYIr838DDx_I4vPrl5msL7hY9ILB83IRlnKseezX9OVDZSBEWcjhivPIPZyg8txP5QrMYutczM7AeNBrUhi8W-rfKdzn1SoC5rd_PklmpUZann-XQYYa1yc_rBL7hUEaLas0fGw9kWpC50GyGFW-tbnQ_7ehpk1lj4M2QLRV3Fmyb-BuRM63M8l9R4a_FWEikzDb-JLl8fGAyw9AQOy_q4f0Q5Q_3hRaHCeFsjJQFdQM3i_9ijeGH05MyrmApQO-PAipaawP9S9OQGNG3ux6T5Ay4aNFBVXA-7v3zi9eGlwMeGJ0kpIfMxcHy-EwwtfUSEXpeXD1fhfCSL8jL8vuGGyWrd-Y5wlMESFlEo1PYpiZRIp78fctCTfeU2Jss9JExK2VhTNr4Ub_h8ojNc7ume7Hxojh1rkLAttg2xw7xC6_hmApy4KY7f__qA7CbbRdiLW3H2aQifMlIFmMfIzaZ2DgEOdofpjynRJ198WsbZrD3CXwDPiV_BnPNpoEqLy6VcUfneL82B5Rs92O3h52sdi4OqtmJzjjWVKcXRYI_9JZoXvnhQ=w1920-h1080-no?authuser=0%22&#34;&gt;https://lh3.googleusercontent.com/AdEHpUUTC1BK7Smsg4jAi3cEadKR6qtvv0GMxtVRui0L83viwc7A6NlZveuzMJI2Hd5d_oBX4gRwMg0_BzbjQaCOa4CbznC8KXe9LiqE4bq4uFZ9OwQsc0dbAPmjCbrxBTOiCxVQ84e8HeqMdkuzqE2IQjImKGnOq0wL4oxwnYIr838DDx_I4vPrl5msL7hY9ILB83IRlnKseezX9OVDZSBEWcjhivPIPZyg8txP5QrMYutczM7AeNBrUhi8W-rfKdzn1SoC5rd_PklmpUZann-XQYYa1yc_rBL7hUEaLas0fGw9kWpC50GyGFW-tbnQ_7ehpk1lj4M2QLRV3Fmyb-BuRM63M8l9R4a_FWEikzDb-JLl8fGAyw9AQOy_q4f0Q5Q_3hRaHCeFsjJQFdQM3i_9ijeGH05MyrmApQO-PAipaawP9S9OQGNG3ux6T5Ay4aNFBVXA-7v3zi9eGlwMeGJ0kpIfMxcHy-EwwtfUSEXpeXD1fhfCSL8jL8vuGGyWrd-Y5wlMESFlEo1PYpiZRIp78fctCTfeU2Jss9JExK2VhTNr4Ub_h8ojNc7ume7Hxojh1rkLAttg2xw7xC6_hmApy4KY7f__qA7CbbRdiLW3H2aQifMlIFmMfIzaZ2DgEOdofpjynRJ198WsbZrD3CXwDPiV_BnPNpoEqLy6VcUfneL82B5Rs92O3h52sdi4OqtmJzjjWVKcXRYI_9JZoXvnhQ=w1920-h1080-no?authuser=0&amp;quot;&lt;/a&gt;
style={{ width: &amp;lsquo;630px&amp;rsquo; }}
/&amp;gt;&lt;/p&gt;
&lt;p&gt;Setting up a code server is rather easy. There are many tutorials on Google search results I have tried that work perfectly. But since I have already joined the cult of Linode, I simply spin up a code server template from Linode&amp;rsquo;s marketplace. The process probably took at most 10 minutes, while the SSL cert was the more PITA thing to do.&lt;/p&gt;
&lt;h2 id=&#34;the-minecraft-server&#34;&gt;The Minecraft server&lt;/h2&gt;
&lt;p&gt;(TBA)&lt;/p&gt;
&lt;h2 id=&#34;the-vpn-servers&#34;&gt;The VPN servers&lt;/h2&gt;
&lt;p&gt;(TBA)&lt;/p&gt;
&lt;h2 id=&#34;the-homelab&#34;&gt;The Homelab&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://linux.hoanganh.tech&#34;&gt;Currently serving a static HTML on &lt;code&gt;linux.hoanganh.tech&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;ssh and xrdp can be accessed through &lt;code&gt;homelab.hoanganh.tech&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hosting a homeserver at home that is accessible to the interweb requires your ISP to provide you with a static IP, which was impossible before at my previous apartment because I was not the internet account holder nor was the router placed in my room. Now, I am able to reserve my own IP to the interweb, route that through Cloudflare et voila.&lt;/p&gt;
&lt;p&gt;In details, it was a bit more complicated. Currently, I have 2 ports forwarded for Remote Desktop Protocol, one for my Linux server GUI and one for my Windows workstation, which are pointed at by 2 CNAMEs configured on Cloudflare: &lt;code&gt;homelab.hoanganh.tech&lt;/code&gt; and &lt;code&gt;home.hoanganh.tech&lt;/code&gt;. Then, a third CNAME, &lt;code&gt;linux.hoanganh.tech&lt;/code&gt;, points to the static web server nginx running on the same Linux server. Introduce the brainfuckery, they are all routed through 1 single IP provided by the Internet Service Provider (ISP). So, for each of these servers/services, I must use separate port. For example, (X)RDP on Linux server is forwarded to port 0001, RDP on Windows is 0002, HTTP on Linux is 80(80), and HTTPS on Linux is 443, and so forth. Took me a while to figure that out and configure on my local machines, on the ISP router portal, and on Cloudflare.&lt;/p&gt;
&lt;h2 id=&#34;the-mail-server&#34;&gt;The mail server&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;mailto:iam@hoanganh.tech&#34;&gt;&lt;code&gt;iam(at)hoanganh(dot)tech&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Initially, I set up the domain with Zoho mail. But because I don&amp;rsquo;t use any of the services that Zoho offers, I migrated everything to Google&amp;rsquo;s G Suite (now Google Workspace). The setup was painless. All I needed to do is copy-paste some MX and DKIM values provided by Google and onto Cloudflare DNS.&lt;/p&gt;
&lt;p&gt;Well, there&amp;rsquo;s always a catch, isn&amp;rsquo;t it? The catch for migration is actually migrating old emails from iCloud and Zoho servers to their new home on Google Workspace. First of all, I have to add the Zoho account to Thunderbird client. Then, I retrieve all the mails from the Zoho account. Finally, I export them to file which I then upload to Gmail. Now, for the iCloud mail, I use the Mail app on macOS to export all mails, which took a day, then upload the exported file to Gmail. Boom. Done.&lt;/p&gt;
&lt;p&gt;The Google Workspace plan that I use is the Business Standard which the following perks:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/U6Yos6bn1ATdLNzTKgvaqNkb11lKri1icGeweG_zcim0HFuKMfNq2yaGYiDSvJd_wDI9yr_EKd5SSZ8X8No26MwqeNK6SsKo5kAESxBwxTfUwakEmcHWBvar6Um__skEa-pl5OR7lx6t6qk3BRPKzyCM8wbagWBEzc4c4VRlPbHsxHnPJxdsrE6M90kUBHrp9lv9fMef5zQQfyvFtjDKcbC9wktWsVNC_lbpBe_bAOrPileSCQxYpKbLF_uCnamaQl3zkfProuxYxs1XuWyrDjwd8w8Z1nL1KG1mTIiUuWr6zMUrDdjJx-GyYicUVoamSf1RowUMmcgsQG-A9k6TAB8Le7YvKtm-wxFJSEB9BEKGzVHT2orhYc6VWWBwZ0ypFoyN4YiFopIFPvuusuVyYM_ywohJhUtn49dyoPGAo0C3cvynbIEVaCvRwE9T0tKPuOxWr9RkuNQIH2ZOF1Kf7Fb-h1tKlH1pPtnjhlBCI_KNRELCvOfHtjHwCL8lChUJQpj4mYyS_g1XHdkQ7MJYZibvKzDKLCmWHz33YHuK7WVYtuJebPVa6luU_NdNFzN-oLB5xp3BkzERStn7HCFUwisgzYxq_9nDYQICQlmM1zY_GvGot3y4OGi4nTzxj_RG7eP5A_-gCibYO4tqFVP1Dj7FJE-NgOCNqxjqQZUOuwY5ctseuh1gHqO3c12Vz-jS51H214LWntSCkGF4kdKjDrsLaA=w576-h1354-no?authuser=0&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;My target is to get the 2TB of cloud storage so that I could store my po&amp;hellip; photos and videos of cats. In all seriousness, mainly important documents and auto HQ photo backup. I might switch to the Business Starter plan which costs half the price, but 2TB is really nice to have.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
